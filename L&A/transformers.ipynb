{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and Large Language Models (LLMs)\n",
    "\n",
    "Transformers are deep learning models designed for sequence processing tasks like:\n",
    "\n",
    "Machine Translation (e.g., Google Translate)\n",
    "\n",
    "Text Generation (e.g., ChatGPT)\n",
    "\n",
    "Question Answering (e.g., BERT)\n",
    "\n",
    "Summarization and Sentiment Analysis\n",
    "\n",
    "They replace RNNs and LSTMs by using self-attention mechanisms to process entire sequences in parallel instead of step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts in Transformers\n",
    "1. Self-Attention Mechanism\n",
    "\n",
    "Allows the model to focus on important words in a sentence, regardless of their position.\n",
    "Example: \"The cat sat on the mat.\" â†’ Focus on \"cat\" when predicting \"sat.\"\n",
    "\n",
    "2. Positional Encoding\n",
    "\n",
    "Adds position information (since transformers process data in parallel and lack inherent order).\n",
    "\n",
    "3. Multi-Head Attention\n",
    "\n",
    "Captures different relationships between words simultaneously.\n",
    "\n",
    "4. Encoder-Decoder Architecture\n",
    "\n",
    "Encoder: Processes input data (e.g., a sentence in English).\n",
    "Decoder: Generates output data (e.g., a sentence in French).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular LLMs (Large Language Models)\n",
    "\n",
    "GPT (Generative Pre-trained Transformer): Text generation and chatbots.\n",
    "\n",
    "BERT (Bidirectional Encoder Representations): Question answering, classification.\n",
    "\n",
    "T5 (Text-to-Text Transfer Transformer): Translation, summarization.\n",
    "\n",
    "LLama (Meta's LLM): Advanced large-scale text generation.\n",
    "\n",
    "Claude (Anthropic): Focused on AI safety and conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "dataset = load_dataset(\"imdb\")  # IMDB movie reviews\n",
    "train_texts = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor(train_labels)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
