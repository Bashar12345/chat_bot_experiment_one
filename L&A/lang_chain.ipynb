{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LnagCahain trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def preprocess_dataset(file_path, relevant_columns):\n",
    "    \"\"\"\n",
    "    Preprocess the Quranic dataset by removing unnecessary columns.\n",
    "    Args:\n",
    "    - file_path: Path to the CSV file.\n",
    "    - relevant_columns: List of columns to retain.\n",
    "\n",
    "    Returns:\n",
    "    - Processed DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df[relevant_columns]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in loading or processing dataset: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if error occurs\n",
    "\n",
    "\n",
    "# Specify relevant columns\n",
    "relevant_columns = ['surah_name_roman', 'surah_name_en', 'ayah_no_surah', 'ayah_en']\n",
    "quran_df = preprocess_dataset(\"TheQuranDataset.csv\", relevant_columns)\n",
    "\n",
    "if quran_df.empty:\n",
    "    print(\"The dataset could not be loaded. Please check the file path and content.\")\n",
    "    exit()  # Exit if the dataset is not properly loaded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Chunk the dataset for LLM processing\n",
    "def preprocess_and_chunk(df, chunk_size=4000):\n",
    "    \"\"\"\n",
    "    Preprocesses the dataset to create formatted context chunks.\n",
    "    Args:\n",
    "    - df: Processed DataFrame.\n",
    "    - chunk_size: Maximum character size for each chunk.\n",
    "\n",
    "    Returns:\n",
    "    - List of context chunks (list of str).\n",
    "    \"\"\"\n",
    "    context_lines = [\n",
    "        f\"Ayah {row['ayah_no_surah']} of Surah {row['surah_name_roman']} ({row['surah_name_en']}): \"\n",
    "        f\"'{row['ayah_en']}'\"\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "    combined_context = \"\\n\".join(context_lines)\n",
    "\n",
    "    # Chunk the combined context\n",
    "    context_chunks = [\n",
    "        combined_context[i:i + chunk_size]\n",
    "        for i in range(0, len(combined_context), chunk_size)\n",
    "    ]\n",
    "    return context_chunks\n",
    "\n",
    "\n",
    "context_chunks = preprocess_and_chunk(quran_df)\n",
    "\n",
    "if not context_chunks:\n",
    "    print(\"No context chunks created. Please check the dataset or preprocessing logic.\")\n",
    "    exit()  # Exit if no chunks are created\n",
    "\n",
    "\n",
    "# Query definition\n",
    "query = \"Does the Quran mention dogs? Provide references and context.\"\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = \"\"\"\n",
    "Use the following Quranic data to answer the query and provide references with context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Query: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create a LangChain PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Create an LLMChain for structured execution\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Execute the query on each chunk\n",
    "def execute_query_on_chunks(context_chunks, query):\n",
    "    \"\"\"\n",
    "    Executes a query on context chunks using the LLM chain.\n",
    "    Args:\n",
    "    - context_chunks: List of text chunks.\n",
    "    - query: Query string.\n",
    "\n",
    "    Returns:\n",
    "    - Combined results from all chunks.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for chunk in context_chunks:\n",
    "        inputs = {\"context\": chunk, \"question\": query}\n",
    "        try:\n",
    "            result = llm_chain.run(inputs)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "\n",
    "final_result = execute_query_on_chunks(context_chunks, query)\n",
    "\n",
    "# Print the final output\n",
    "print(\"\\nOutput:\")\n",
    "print(final_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Keyword search function\n",
    "def keyword_search(keywords, df):\n",
    "    \"\"\"\n",
    "    Searches for ayahs containing specific keywords in the Quranic dataset.\n",
    "    Args:\n",
    "    - keywords: List of keywords to search for.\n",
    "    - df: DataFrame containing Quranic data.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with matched ayahs and relevant details.\n",
    "    \"\"\"\n",
    "    matches = df[\n",
    "        df['ayah_en'].str.contains('|'.join(keywords), case=False, na=False)\n",
    "    ]\n",
    "    return matches\n",
    "\n",
    "\n",
    "# Keywords related to \"dogs\"\n",
    "keywords = [\"dog\", \"dogs\", \"dogs talk\", \"speak\"]\n",
    "matched_ayahs = keyword_search(keywords, quran_df)\n",
    "\n",
    "# Display the results in a clean table format\n",
    "if not matched_ayahs.empty:\n",
    "    print(\"\\nMatched Ayahs:\")\n",
    "    print(\n",
    "        matched_ayahs.to_markdown(index=False)  # Use Markdown for table-like display\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo matches found for the given keywords.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Basic LangChain Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize OpenAI model\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.7)\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"\n",
    "You are a helpful assistant. Answer the user's question based on the document provided.\n",
    "\n",
    "Document: {document}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"document\", \"question\"])\n",
    "\n",
    "# Build the chain\n",
    "qa_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Provide a sample document and question\n",
    "document = \"Artificial Intelligence (AI) involves creating systems capable of performing tasks that typically require human intelligence.\"\n",
    "question = \"What is AI?\"\n",
    "\n",
    "# Run the chain\n",
    "response = qa_chain.run({\"document\": document, \"question\": question})\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Retrieval with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"AI is transforming industries.\",\n",
    "    \"AI applications include NLP and robotics.\",\n",
    "    \"Machine learning is a subset of AI.\"\n",
    "]\n",
    "\n",
    "# Create embeddings\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "vector_store = Chroma.from_texts(documents, embedding=embedding_model)\n",
    "\n",
    "# Query the vector store\n",
    "query = \"Tell me about AI applications.\"\n",
    "results = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "print(\"Retrieved Documents:\")\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a conversational chain\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "# Simulate a conversation\n",
    "response1 = conversation.predict(input=\"What is AI?\")\n",
    "response2 = conversation.predict(input=\"Tell me about its applications.\")\n",
    "\n",
    "print(\"Conversation:\")\n",
    "print(response1)\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Define a tool\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "tools = [Tool(name=\"Calculator\", func=calculator, description=\"Performs calculations.\")]\n",
    "\n",
    "# Initialize an agent\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "# Query the agent\n",
    "response = agent.run(\"What is 12 multiplied by 8?\")\n",
    "print(\"Agent Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Document Retrieval Pipelines with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Combine retrieval with QA\n",
    "retrieval_qa = RetrievalQA.from_chain_type(llm=llm, retriever=vector_store.as_retriever())\n",
    "\n",
    "# Ask a question\n",
    "question = \"What is AI?\"\n",
    "response = retrieval_qa.run(question)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Locally or on the Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the conversational chain\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "conversation = ConversationChain(llm=llm)\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Conversational AI with LangChain\")\n",
    "user_input = st.text_input(\"You: \", \"\")\n",
    "if user_input:\n",
    "    response = conversation.predict(input=user_input)\n",
    "    st.text_area(\"AI:\", response, height=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.tools import GoogleSearchRun\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Step 1: Prompt Template\n",
    "template = \"\"\"You are an intelligent assistant. Here is the user's question:\n",
    "{question}\n",
    "\n",
    "Provide a concise, helpful response:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=template)\n",
    "\n",
    "# Step 2: Memory for Context\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Step 3: LLM (Language Model)\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0.3)\n",
    "\n",
    "# Step 4: Vector Database for Document Retrieval\n",
    "# Create a small set of documents\n",
    "documents = [\n",
    "    \"LangChain is a framework for building applications with language models.\",\n",
    "    \"OpenAI's GPT-3 is a widely used language model for text generation.\",\n",
    "    \"LangChain enables retrieval-augmented generation (RAG) with embeddings.\",\n",
    "]\n",
    "\n",
    "# Generate embeddings and create a vectorstore\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_texts(documents, embedding)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Step 5: Define Tools\n",
    "search_tool = Tool(\n",
    "    name=\"GoogleSearch\",\n",
    "    func=GoogleSearchRun().run,\n",
    "    description=\"Search Google for information on any topic.\"\n",
    ")\n",
    "\n",
    "# Step 6: Conversational Retrieval Chain\n",
    "retrieval_chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Step 7: Create an Agent with Tools\n",
    "tools = [search_tool]\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", memory=memory)\n",
    "\n",
    "# Step 8: LLM Chain for Prompt Template\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Step 9: Run the Workflow\n",
    "# 1. Use the conversational retrieval chain to retrieve knowledge\n",
    "query = \"What is LangChain?\"\n",
    "retrieval_response = retrieval_chain.run(query)\n",
    "print(f\"[Retrieved Knowledge]: {retrieval_response}\")\n",
    "\n",
    "# 2. Use the LLM chain with the prompt template\n",
    "question = \"What is the difference between GPT-3 and LangChain?\"\n",
    "formatted_response = llm_chain.run({\"question\": question})\n",
    "print(f\"[LLM Response with Prompt]: {formatted_response}\")\n",
    "\n",
    "# 3. Use the agent with tools to get external information\n",
    "agent_query = \"Who is the CEO of OpenAI?\"\n",
    "agent_response = agent.run(agent_query)\n",
    "print(f\"[Agent Response]: {agent_response}\")\n",
    "\n",
    "# 4. Demonstrate memory by asking a follow-up question\n",
    "follow_up = \"What did I just ask you about OpenAI?\"\n",
    "follow_up_response = agent.run(follow_up)\n",
    "print(f\"[Follow-Up with Memory]: {follow_up_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.tools import GoogleSearchRun\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "class LangChainChatbot:\n",
    "    def __init__(self):\n",
    "        # Step 1: Initialize LLM\n",
    "        self.llm = OpenAI(model=\"text-davinci-003\", temperature=0.7)\n",
    "\n",
    "        # Step 2: Initialize Memory\n",
    "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "        # Step 3: Initialize Prompt Template\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"chat_history\", \"user_input\"],\n",
    "            template=(\n",
    "                \"You are a helpful chatbot. Here's the conversation so far:\\n\"\n",
    "                \"{chat_history}\\n\\n\"\n",
    "                \"User: {user_input}\\nChatbot:\"\n",
    "            )\n",
    "        )\n",
    "        self.llm_chain = LLMChain(llm=self.llm, prompt=self.prompt_template)\n",
    "\n",
    "        # Step 4: Initialize Document Retrieval (RAG)\n",
    "        documents = [\n",
    "            \"LangChain is a framework for building applications with language models.\",\n",
    "            \"OpenAI's GPT-3 is a popular model for text generation.\",\n",
    "            \"LangChain supports tools like embeddings, memory, and agents for reasoning.\",\n",
    "        ]\n",
    "        embedding = OpenAIEmbeddings()\n",
    "        self.vectorstore = Chroma.from_texts(documents, embedding)\n",
    "        self.retriever = self.vectorstore.as_retriever()\n",
    "\n",
    "        # Step 5: Initialize Tools\n",
    "        search_tool = Tool(\n",
    "            name=\"GoogleSearch\",\n",
    "            func=GoogleSearchRun().run,\n",
    "            description=\"Search Google for information on any topic.\"\n",
    "        )\n",
    "        self.tools = [search_tool]\n",
    "\n",
    "        # Step 6: Initialize Agent\n",
    "        self.agent = initialize_agent(\n",
    "            tools=self.tools,\n",
    "            llm=self.llm,\n",
    "            agent=\"zero-shot-react-description\",\n",
    "            memory=self.memory\n",
    "        )\n",
    "\n",
    "        # Step 7: Initialize Conversational Retrieval Chain\n",
    "        self.retrieval_chain = ConversationalRetrievalChain(\n",
    "            retriever=self.retriever,\n",
    "            memory=self.memory,\n",
    "            llm=self.llm\n",
    "        )\n",
    "\n",
    "    def chat(self, user_input):\n",
    "        \"\"\"\n",
    "        Handle a single user input and decide whether to use retrieval, tools, or chains.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # If the user input is a retrieval-based query\n",
    "            if \"retrieve\" in user_input.lower():\n",
    "                response = self.retrieval_chain.run(user_input)\n",
    "                return f\"[Retrieved Info]: {response}\"\n",
    "\n",
    "            # If the user input requires tool usage (e.g., Google Search)\n",
    "            elif \"search\" in user_input.lower():\n",
    "                response = self.agent.run(user_input)\n",
    "                return f\"[Search Result]: {response}\"\n",
    "\n",
    "            # Default response using prompt templating and memory\n",
    "            else:\n",
    "                response = self.llm_chain.run({\n",
    "                    \"chat_history\": self.memory.load_memory_variables({}).get(\"chat_history\", \"\"),\n",
    "                    \"user_input\": user_input\n",
    "                })\n",
    "                return response\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {e}\"\n",
    "\n",
    "# Main Chatbot Loop\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = LangChainChatbot()\n",
    "    print(\"Chatbot is ready! Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = chatbot.chat(user_input)\n",
    "        print(f\"Chatbot: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
