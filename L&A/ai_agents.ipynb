{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planner: Created tasks for 'Artificial Intelligence'\n",
      "Retriever: Fetching articles for 'Artificial Intelligence'...\n",
      "Executor: Extracting key points...\n",
      "Executor: Summarizing key points...\n",
      "Final Output: Summary: AI is transforming industries like healthcare and finance. AI advancements include NLP, computer vision, and robotics.\n"
     ]
    }
   ],
   "source": [
    "class PlannerAgent:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "\n",
    "    def plan_task(self, topic):\n",
    "        self.tasks = [\n",
    "            {\"task\": \"Search for articles about the topic\", \"agent\": \"Retriever\"},\n",
    "            {\"task\": \"Extract key points from articles\", \"agent\": \"Executor\"},\n",
    "            {\"task\": \"Summarize the extracted points\", \"agent\": \"Executor\"},\n",
    "        ]\n",
    "        print(f\"Planner: Created tasks for '{topic}'\")\n",
    "        return self.tasks\n",
    "\n",
    "\n",
    "class RetrieverAgent:\n",
    "    def retrieve_data(self, topic):\n",
    "        # Simulate data retrieval (replace with actual web scraping or API calls)\n",
    "        print(f\"Retriever: Fetching articles for '{topic}'...\")\n",
    "        return [\n",
    "            \"Article 1: AI is transforming industries like healthcare and finance.\",\n",
    "            \"Article 2: AI advancements include NLP, computer vision, and robotics.\",\n",
    "        ]\n",
    "\n",
    "\n",
    "class ExecutorAgent:\n",
    "    def extract_key_points(self, articles):\n",
    "        print(\"Executor: Extracting key points...\")\n",
    "        key_points = []\n",
    "        for article in articles:\n",
    "            key_points.append(article.split(\": \")[1])  # Simulated extraction\n",
    "        return key_points\n",
    "\n",
    "    def summarize(self, key_points):\n",
    "        print(\"Executor: Summarizing key points...\")\n",
    "        return f\"Summary: {' '.join(key_points)}\"\n",
    "\n",
    "\n",
    "# Centralized Communication\n",
    "class CentralizedCoordinator:\n",
    "    def __init__(self):\n",
    "        self.planner = PlannerAgent()\n",
    "        self.retriever = RetrieverAgent()\n",
    "        self.executor = ExecutorAgent()\n",
    "\n",
    "    def execute_task(self, topic):\n",
    "        tasks = self.planner.plan_task(topic)\n",
    "\n",
    "        for task in tasks:\n",
    "            if task[\"agent\"] == \"Retriever\":\n",
    "                articles = self.retriever.retrieve_data(topic)\n",
    "            elif task[\"agent\"] == \"Executor\" and \"Extract\" in task[\"task\"]:\n",
    "                key_points = self.executor.extract_key_points(articles)\n",
    "            elif task[\"agent\"] == \"Executor\" and \"Summarize\" in task[\"task\"]:\n",
    "                summary = self.executor.summarize(key_points)\n",
    "\n",
    "        print(f\"Final Output: {summary}\")\n",
    "\n",
    "\n",
    "# Run the system\n",
    "if __name__ == \"__main__\":\n",
    "    topic = \"Artificial Intelligence\"\n",
    "    coordinator = CentralizedCoordinator()\n",
    "    coordinator.execute_task(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannerAgent:\n",
    "    def __init__(self, model_name=\"Qwen/Qwen2.5-1.5B-Instruct\"):\n",
    "        from transformers import pipeline\n",
    "        self.planner = pipeline(\n",
    "            \"text-generation\", \n",
    "            model=model_name, \n",
    "            max_length=150, \n",
    "            num_return_sequences=1, \n",
    "            temperature=0.3, \n",
    "            top_k=10, \n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "    def plan_task(self, input_task):\n",
    "        response = self.planner(\n",
    "            f\"Please break down the following task into a clear and concise list of subtasks: {input_task}\"\n",
    "        )\n",
    "        if not response or 'generated_text' not in response[0]:\n",
    "            raise ValueError(\"Model did not generate a valid response.\")\n",
    "        \n",
    "        generated_text = response[0]['generated_text'].strip()\n",
    "        print(\"Raw response from model:\", generated_text)  # Debugging\n",
    "        \n",
    "        subtasks = generated_text.split(\"\\n\")\n",
    "        return [task.strip() for task in subtasks if task.strip()]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RetrieverAgent:\n",
    "    def __init__(self, model_name=\"deepset/roberta-base-squad2\"):\n",
    "        from transformers import pipeline\n",
    "        self.retriever = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=model_name,\n",
    "            top_k=5,  # Retrieve up to 5 answers\n",
    "            handle_impossible_answer=True,\n",
    "        )\n",
    "\n",
    "    def retrieve_data(self, question, context):\n",
    "        max_context_length = 512\n",
    "        if len(context.split()) > max_context_length:\n",
    "            context_chunks = self._split_context(context, max_context_length)\n",
    "            answers = []\n",
    "            for chunk in context_chunks:\n",
    "                try:\n",
    "                    response = self.retriever(question=question, context=chunk)\n",
    "                    answers.extend(self._process_response(response))\n",
    "                except Exception as e:\n",
    "                    answers.append(f\"Error: {str(e)}\")\n",
    "            return \" \".join(answers).strip()\n",
    "        else:\n",
    "            response = self.retriever(question=question, context=context)\n",
    "            return \" \".join(self._process_response(response))\n",
    "\n",
    "    def _process_response(self, response):\n",
    "        \"\"\"Process the response list and extract answers.\"\"\"\n",
    "        if not response:\n",
    "            return [\"No relevant data found for this subtask.\"]\n",
    "        answers = []\n",
    "        for item in response:\n",
    "            answer = item.get(\"answer\", \"\").strip()\n",
    "            if answer:\n",
    "                answers.append(answer)\n",
    "        return answers if answers else [\"No relevant data found for this subtask.\"]\n",
    "\n",
    "    def _split_context(self, context, max_length):\n",
    "        words = context.split()\n",
    "        return [\n",
    "            \" \".join(words[i:i + max_length])\n",
    "            for i in range(0, len(words), max_length)\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizerAgent:\n",
    "    def __init__(self, model_name=\"facebook/bart-large-cnn\"):\n",
    "        from transformers import pipeline\n",
    "        self.summarizer = pipeline(\"summarization\", model=model_name)\n",
    "\n",
    "    def _split_text(self, text, max_tokens=1024):\n",
    "        \"\"\"Split text into chunks if it exceeds the model's token limit.\"\"\"\n",
    "        words = text.split()\n",
    "        return [\n",
    "            \" \".join(words[i:i + max_tokens])\n",
    "            for i in range(0, len(words), max_tokens)\n",
    "        ]\n",
    "\n",
    "    def summarize(self, text, max_length=130, min_length=30, do_sample=False):\n",
    "        \"\"\"Summarize the input text, handling edge cases and long inputs.\"\"\"\n",
    "        try:\n",
    "            # Handle empty or whitespace-only text\n",
    "            if not text.strip():\n",
    "                return \"The input text is empty or contains only whitespace.\"\n",
    "\n",
    "            # Debugging input text\n",
    "            print(f\"Input text: '{text}'\")\n",
    "            print(f\"Word count: {len(text.split())}\")\n",
    "\n",
    "            # Handle short texts\n",
    "            if len(text.split()) < 30:\n",
    "                return \"The text is too short to summarize meaningfully.\"\n",
    "\n",
    "            # Handle long texts by splitting and summarizing chunks\n",
    "            if len(text.split()) > 1024:\n",
    "                chunks = self._split_text(text, max_tokens=1024)\n",
    "                summaries = []\n",
    "                for chunk in chunks:\n",
    "                    response = self.summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=do_sample)\n",
    "                    summaries.append(response[0][\"summary_text\"])\n",
    "                return \" \".join(summaries)\n",
    "\n",
    "            # Summarize normally for medium-length texts\n",
    "            response = self.summarizer(text, max_length=max_length, min_length=min_length, do_sample=do_sample)\n",
    "            print(\"Raw summarizer response:\", response)  # Debugging\n",
    "            if not response or \"summary_text\" not in response[0]:\n",
    "                return \"No summary could be generated.\"\n",
    "            return response[0][\"summary_text\"]\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error in summarization: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoleManager:\n",
    "    def __init__(self):\n",
    "        self.planner = PlannerAgent()\n",
    "        self.retriever = RetrieverAgent()\n",
    "        self.summarizer = SummarizerAgent()\n",
    "\n",
    "    def execute(self, input_task, context):\n",
    "        # Step 1: Plan the task\n",
    "        subtasks = self.planner.plan_task(input_task)\n",
    "        # print(f\"Subtasks:\\n {subtasks}\")\n",
    "\n",
    "        # Step 2: Retrieve data for each subtask\n",
    "        retrieved_data = []\n",
    "        # for task in subtasks:\n",
    "        #     data = self.retriever.retrieve_data(task, context)\n",
    "        #     retrieved_data.append(data)\n",
    "        \n",
    "        \n",
    "        def simplify_task(task):\n",
    "            # Remove verbose instructions and keep the core question\n",
    "            return task.split(\":\")[-1].strip()\n",
    "                \n",
    "        for task in subtasks:\n",
    "            simple_task = simplify_task(task).strip()\n",
    "            if not simple_task:\n",
    "                print(\"Skipping empty or invalid subtask.\")\n",
    "                continue\n",
    "            print(f\"Simplified subtask: '{simple_task}'\")\n",
    "            data = self.retriever.retrieve_data(simple_task, context)\n",
    "            retrieved_data.append(data)\n",
    "\n",
    "            \n",
    "        print(f\"Orginal context: {context}\")    \n",
    "        print(f\"Retrieved data for '{retrieved_data}\")\n",
    "\n",
    "        # Step 3: Summarize the data\n",
    "        full_text = \" \".join(retrieved_data)\n",
    "        print(f\"Full text: {full_text}\")\n",
    "        summary = self.summarizer.summarize(full_text)\n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vai/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-05 17:35:51.647881: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-05 17:35:51.844439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736076951.925769  137936 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736076951.945996  137936 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-05 17:35:52.089683: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response from model: Please break down the following task into a clear and concise list of subtasks: Summarize the advancements in Internet Technology. To summarize the advancements in Internet Technology, you can follow these steps:\n",
      "\n",
      "1. Identify key technologies that have been developed or improved over time.\n",
      "2. Analyze how these technologies have impacted various aspects of society, such as communication, commerce, education, and entertainment.\n",
      "3. Consider any new developments or innovations that are currently being researched or tested.\n",
      "4. Evaluate the potential future impact of these advancements on society and technology.\n",
      "\n",
      "By breaking down the task into these subtasks, it becomes easier to approach and complete the summarization process systematically. Each step involves analyzing specific areas within Internet Technology and considering their broader implications for society and technology\n",
      "Skipping empty or invalid subtask.\n",
      "Simplified subtask: '1. Identify key technologies that have been developed or improved over time.'\n",
      "Simplified subtask: '2. Analyze how these technologies have impacted various aspects of society, such as communication, commerce, education, and entertainment.'\n",
      "Simplified subtask: '3. Consider any new developments or innovations that are currently being researched or tested.'\n",
      "Simplified subtask: '4. Evaluate the potential future impact of these advancements on society and technology.'\n",
      "Simplified subtask: 'By breaking down the task into these subtasks, it becomes easier to approach and complete the summarization process systematically. Each step involves analyzing specific areas within Internet Technology and considering their broader implications for society and technology'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal context: \n",
      "The internet, a revolutionary technology, began as a research project in the 1960s with the creation of ARPANET, a U.S. Department of Defense initiative. The invention of the World Wide Web by Tim Berners-Lee in 1989 transformed the internet into a global communication tool. The 1990s saw rapid expansion with the rise of email, search engines, and e-commerce platforms like Amazon and eBay. Social media platforms like Facebook, Twitter, and Instagram reshaped how people connect and share information in the 2000s. Today, the internet is an integral part of daily life, enabling everything from online education to virtual reality experiences.\n",
      "    \n",
      "Retrieved data for '['internet The internet The internet', 'Social media platforms Social media platforms like Facebook, Twitter, and Instagram reshaped how people connect and share information reshaped how people connect and share information in the 2000s.', 'The internet internet The internet The internet, a revolutionary technology', 'online education to virtual reality experiences virtual reality experiences Social media platforms the internet is an integral part of daily life', 'No relevant data found for this subtask.']\n",
      "Full text: internet The internet The internet Social media platforms Social media platforms like Facebook, Twitter, and Instagram reshaped how people connect and share information reshaped how people connect and share information in the 2000s. The internet internet The internet The internet, a revolutionary technology online education to virtual reality experiences virtual reality experiences Social media platforms the internet is an integral part of daily life No relevant data found for this subtask.\n",
      "Input text: 'internet The internet The internet Social media platforms Social media platforms like Facebook, Twitter, and Instagram reshaped how people connect and share information reshaped how people connect and share information in the 2000s. The internet internet The internet The internet, a revolutionary technology online education to virtual reality experiences virtual reality experiences Social media platforms the internet is an integral part of daily life No relevant data found for this subtask.'\n",
      "Word count: 71\n",
      "Raw summarizer response: [{'summary_text': 'Social media platforms like Facebook, Twitter, and Instagram reshaped how people connect and share information in the 2000s. No relevant data found for this subtask.'}]\n",
      "\n",
      "[Final Output]: Social media platforms like Facebook, Twitter, and Instagram reshaped how people connect and share information in the 2000s. No relevant data found for this subtask.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Context data for retrieval (e.g., an article or knowledge base)\n",
    "    context = \"\"\"\n",
    "The internet, a revolutionary technology, began as a research project in the 1960s with the creation of ARPANET, a U.S. Department of Defense initiative. The invention of the World Wide Web by Tim Berners-Lee in 1989 transformed the internet into a global communication tool. The 1990s saw rapid expansion with the rise of email, search engines, and e-commerce platforms like Amazon and eBay. Social media platforms like Facebook, Twitter, and Instagram reshaped how people connect and share information in the 2000s. Today, the internet is an integral part of daily life, enabling everything from online education to virtual reality experiences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input task\n",
    "    task = \"Summarize the advancements in Internet Technology.\"\n",
    "\n",
    "    # Run the multi-agent system\n",
    "    manager = RoleManager()\n",
    "    result = manager.execute(task, context)\n",
    "    print(\"\\n[Final Output]:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RetrieverAgent:\n",
    "#     def __init__(self, model_name=\"deepset/roberta-base-squad2\"):\n",
    "#         from transformers import pipeline\n",
    "#         self.retriever = pipeline(\n",
    "#             \"question-answering\",\n",
    "#             model=model_name,\n",
    "#             top_k=5,\n",
    "#             handle_impossible_answer=True,\n",
    "#         )\n",
    "\n",
    "#     def retrieve_data(self, question, context):\n",
    "#         max_context_length = 512\n",
    "#         if len(context.split()) > max_context_length:\n",
    "#             context_chunks = self._split_context(context, max_context_length)\n",
    "#             answers = []\n",
    "#             for chunk in context_chunks:\n",
    "#                 try:\n",
    "#                     response = self.retriever(question=question, context=chunk)\n",
    "#                     answer = response.get(\"answer\", \"\").strip()\n",
    "#                     if not answer:\n",
    "#                         answer = \"No relevant data found for this subtask.\"\n",
    "#                     answers.append(answer)\n",
    "#                 except Exception as e:\n",
    "#                     answers.append(f\"Error: {str(e)}\")\n",
    "#             return \" \".join(answers).strip()\n",
    "#         else:\n",
    "#             response = self.retriever(question=question, context=context)\n",
    "#             answer = response.get(\"answer\", \"\").strip()\n",
    "#             return answer if answer else \"No relevant data found for this subtask.\"\n",
    "\n",
    "#     def _split_context(self, context, max_length):\n",
    "#         words = context.split()\n",
    "#         return [\n",
    "#             \" \".join(words[i : i + max_length])\n",
    "#             for i in range(0, len(words), max_length)\n",
    "#         ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
