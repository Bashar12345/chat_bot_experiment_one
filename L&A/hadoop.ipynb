{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Overview\n",
    "Apache Hadoop is an open-source framework for storing and processing large datasets in a distributed computing environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components of Hadoop\n",
    "#### HDFS (Hadoop Distributed File System)\n",
    "\n",
    "#### Distributed storage system for big data.\n",
    "Data is split into blocks and stored across multiple nodes.\n",
    "MapReduce\n",
    "\n",
    "#### Processing framework for parallel computation.\n",
    "#### Map: Processes and filters data.\n",
    "#### Reduce: Aggregates results.\n",
    "YARN (Yet Another Resource Negotiator)\n",
    "\n",
    "Resource manager for handling distributed tasks.\n",
    "Hadoop Common\n",
    "\n",
    "Libraries and utilities supporting other modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start-dfs.sh\n",
    "start-yarn.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Hadoop Works\n",
    "Input Data → Split into blocks (e.g., 128 MB each).\n",
    "\n",
    "Distributed Storage → Blocks stored across multiple nodes in HDFS.\n",
    "\n",
    "Parallel Processing → MapReduce processes each block simultaneously.\n",
    "\n",
    "Fault Tolerance → Replicates data blocks for reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Files in HDFS:\n",
    "\n",
    "hdfs dfs -ls /\n",
    "\n",
    "# Copy Local File to HDFS:\n",
    "\n",
    "hdfs dfs -put file.txt /data/\n",
    "\n",
    "# View File Content in HDFS:\n",
    "\n",
    "hdfs dfs -cat /data/file.txt\n",
    "\n",
    "# Run Word Count Example (MapReduce):\n",
    "\n",
    "hadoop jar hadoop-mapreduce-examples.jar wordcount /input /output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Use Cases\n",
    "\n",
    "Data Warehousing: Storing massive datasets for analytics.\n",
    "\n",
    "Log Processing: Analyzing logs from servers and applications.\n",
    "\n",
    "Recommendation Systems: Building product recommendations.\n",
    "\n",
    "Sentiment Analysis: Processing social media data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Components of Spark\n",
    "#### Spark Core\n",
    "\n",
    "Basic functionalities: task scheduling, memory management, fault recovery.\n",
    "Provides the RDD (Resilient Distributed Dataset) API for distributed data processing.\n",
    "\n",
    "Spark SQL\n",
    "\n",
    "Allows querying structured data using SQL.\n",
    "Supports DataFrames for data manipulation.\n",
    "\n",
    "Spark Streaming\n",
    "\n",
    "Processes real-time data streams from sources like Kafka and Flume.\n",
    "\n",
    "MLlib (Machine Learning Library)\n",
    "\n",
    "Built-in tools for machine learning and statistical analysis.\n",
    "\n",
    "GraphX\n",
    "\n",
    "For graph processing and analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName('SparkExample').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
    "\n",
    "# View data\n",
    "data.show()\n",
    "\n",
    "# Select columns\n",
    "data.select('column1', 'column2').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with RDDs\n",
    "Create RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# View data\n",
    "print(rdd.collect())\n",
    "\n",
    "\n",
    "# Transformations and Actions\n",
    "\n",
    "# Map Transformation\n",
    "rdd_map = rdd.map(lambda x: (x[0], x[1] + 10))\n",
    "\n",
    "# Filter Transformation\n",
    "rdd_filter = rdd.filter(lambda x: x[1] > 30)\n",
    "\n",
    "# Actions\n",
    "print(rdd_map.collect())\n",
    "print(rdd_filter.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL with DataFrames\n",
    "### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [Row(name=\"Alice\", age=34), Row(name=\"Bob\", age=45)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Show Data\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "result = spark.sql(\"SELECT * FROM people WHERE age > 35\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with MLlib\n",
    "##### Example: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Prepare Data\n",
    "data = [(1, 2.0), (2, 4.0), (3, 6.0)]\n",
    "df = spark.createDataFrame(data, [\"feature\", \"label\"])\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=[\"feature\"], outputCol=\"features\")\n",
    "output = assembler.transform(df)\n",
    "\n",
    "# Train Model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(output)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(output)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cases for Spark\n",
    "#### ETL (Extract, Transform, Load):\n",
    "\n",
    "Process and clean large datasets efficiently.\n",
    "\n",
    "#### Real-time Analytics:\n",
    "\n",
    "Stream processing for IoT data or logs.\n",
    "\n",
    "#### Machine Learning Pipelines:\n",
    "\n",
    "Train scalable ML models with large datasets.\n",
    "\n",
    "#### Graph Processing:\n",
    "\n",
    "Social network analysis or recommendation engines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
