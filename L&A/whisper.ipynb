{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transcribe audio to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000  # Wav2Vec2 expects 16kHz audio\n",
    "CHUNK = 1024  # Number of audio samples per frame\n",
    "\n",
    "# Load the Wav2Vec2 model and processor\n",
    "print(\"Loading Wav2Vec2 model...\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "stream = audio.open(format=FORMAT, \n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"Recording and transcribing in real-time... Press Ctrl+C to stop.\")\n",
    "\n",
    "# Buffer for real-time processing\n",
    "audio_buffer = np.zeros((RATE * 5,), dtype=np.int16)  # 5-second sliding window\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Read audio data from the microphone\n",
    "        data = stream.read(CHUNK)\n",
    "        audio_samples = np.frombuffer(data, dtype=np.int16)\n",
    "\n",
    "        # Add new samples to the buffer and keep only the last 5 seconds\n",
    "        audio_buffer = np.concatenate((audio_buffer, audio_samples))[-RATE * 5:]\n",
    "\n",
    "        # Normalize and prepare input features\n",
    "        input_values = processor(\n",
    "            audio_buffer, \n",
    "            sampling_rate=RATE, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True\n",
    "        ).input_values\n",
    "\n",
    "        # Move input values to the appropriate device\n",
    "        input_values = input_values.to(device)\n",
    "\n",
    "        # Perform transcription\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "            transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "        # Print the transcription in real-time\n",
    "        print(f\"\\rTranscription: {transcription}\", end=\"\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTranscription stopped.\")\n",
    "finally:\n",
    "    # Stop and clean up\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    print(\"\\nRecording and transcription stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /home/vai/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages (1.14.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /home/vai/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages (from scipy) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n",
      "Using device 'HDA Intel PCH: ALC897 Analog (hw:0,0)' at 44100 Hz.\n",
      "Recording and transcribing in real-time... Press 'S' to stop.\n",
      "Transcription:  You not sure if you can see it.-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아ng f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing f***ing fng up to f***ing up to f***ing up to f***ing up to f***"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.signal import resample\n",
    "from pynput import keyboard\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Suppress TensorFlow and other warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"openai/whisper-tiny\"  # Use a smaller model for faster transcription\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHUNK = 2048  # Increased buffer size to prevent overflow\n",
    "DEVICE_INDEX = 0  # Replace with the correct device index\n",
    "RATE = 44100  # Default sample rate of your device\n",
    "TARGET_RATE = 16000  # Whisper's required sample rate\n",
    "CHANNELS = 1  # Use mono input for transcription\n",
    "\n",
    "# Load Whisper model and processor\n",
    "print(\"Loading Whisper model...\")\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "stream = audio.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    input_device_index=DEVICE_INDEX,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "print(f\"Using device '{audio.get_device_info_by_index(DEVICE_INDEX)['name']}' at {RATE} Hz.\")\n",
    "print(\"Recording and transcribing in real-time... Press 'S' to stop.\")\n",
    "\n",
    "# Real-time buffer\n",
    "audio_buffer = np.zeros((TARGET_RATE * 5,), dtype=np.int16)  # 5-second sliding window\n",
    "recording = True\n",
    "\n",
    "# Handle key press to stop transcription\n",
    "def on_press(key):\n",
    "    global recording\n",
    "    try:\n",
    "        if key.char == 's':  # Stop on 'S' key press\n",
    "            print(\"\\n'S' key pressed. Stopping transcription...\")\n",
    "            recording = False\n",
    "            return False\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "listener = keyboard.Listener(on_press=on_press)\n",
    "listener.start()\n",
    "\n",
    "try:\n",
    "    while recording:\n",
    "        try:\n",
    "            # Read audio data\n",
    "            data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "            audio_samples = np.frombuffer(data, dtype=np.int16)\n",
    "\n",
    "            # Resample from 44100 Hz to 16000 Hz\n",
    "            resampled_audio = resample(audio_samples, int(len(audio_samples) * TARGET_RATE / RATE))\n",
    "\n",
    "            # Add to audio buffer\n",
    "            audio_buffer = np.concatenate((audio_buffer, resampled_audio))[-TARGET_RATE * 5:]\n",
    "\n",
    "            # Normalize audio and process with Whisper\n",
    "            input_features = processor(\n",
    "                audio_buffer.astype(np.float32) / 32768.0,  # Normalize int16 to float32\n",
    "                sampling_rate=TARGET_RATE,\n",
    "                return_tensors=\"pt\",\n",
    "                language=\"en\"  # Set language for transcription\n",
    "            ).input_features\n",
    "\n",
    "            input_features = input_features.to(device)\n",
    "            with torch.no_grad():\n",
    "                predicted_ids = model.generate(input_features)\n",
    "            transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "            # Print the transcription\n",
    "            print(f\"\\rTranscription: {transcription}\", end=\"\")\n",
    "        except OSError as e:\n",
    "            if e.errno == -9981:  # Input overflow error\n",
    "                print(\"\\nWarning: Input overflowed. Skipping this chunk.\")\n",
    "                continue\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "finally:\n",
    "    # Cleanup\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    listener.join()\n",
    "    print(\"\\nTranscription stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "p = pyaudio.PyAudio()\n",
    "for i in range(p.get_device_count()):\n",
    "    print(p.get_device_info_by_index(i))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
