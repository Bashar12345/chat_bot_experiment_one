{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accent Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open a stream for real-time audio capture\n",
    "stream = audio.open(format=pyaudio.paInt16, \n",
    "                    channels=1,\n",
    "                    rate=16000,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=1024)\n",
    "\n",
    "print(\"Recording...\")\n",
    "while True:\n",
    "    data = stream.read(1024)\n",
    "    # Process audio data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = \"your_openai_api_key\"\n",
    "\n",
    "# Transcribe audio using OpenAI API\n",
    "audio_file = open(\"media/recorded_audio.wav\", \"rb\")\n",
    "transcription = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "print(\"Transcription:\", transcription[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: \n",
      "Transcription: how much time\n",
      "Transcription:  hello can you hear me\n",
      "Transcription:  okay stop listening and give me the output\n",
      "Transcription:  it's perfect right\n",
      "Transcription:  so what do you think about it\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscription: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39malternatives[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtranscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mreal_time_transcription\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m, in \u001b[0;36mreal_time_transcription\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m responses \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mstreaming_recognize(config\u001b[38;5;241m=\u001b[39mstreaming_config, requests\u001b[38;5;241m=\u001b[39mrequests)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Print transcriptions\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_final\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:116\u001b[0m, in \u001b[0;36m_StreamingResponseIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stored_first_result\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# If the stream has already returned data, we cannot recover here.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/grpc/_channel.py:543\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/grpc/_channel.py:960\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_response_ready\u001b[39m():\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    956\u001b[0m         cygrpc\u001b[38;5;241m.\u001b[39mOperationType\u001b[38;5;241m.\u001b[39mreceive_message \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mdue\n\u001b[1;32m    957\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     )\n\u001b[0;32m--> 960\u001b[0m \u001b[43m_common\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_response_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/grpc/_common.py:156\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(wait_fn, wait_complete_fn, timeout, spin_cb)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait_complete_fn():\n\u001b[0;32m--> 156\u001b[0m         \u001b[43m_wait_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAXIMUM_WAIT_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspin_cb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m timeout\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/grpc/_common.py:116\u001b[0m, in \u001b[0;36m_wait_once\u001b[0;34m(wait_fn, timeout, spin_cb)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait_once\u001b[39m(\n\u001b[1;32m    112\u001b[0m     wait_fn: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mbool\u001b[39m],\n\u001b[1;32m    113\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    114\u001b[0m     spin_cb: Optional[Callable[[], \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[1;32m    115\u001b[0m ):\n\u001b[0;32m--> 116\u001b[0m     \u001b[43mwait_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spin_cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m         spin_cb()\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "from google.cloud import speech\n",
    "\n",
    "def real_time_transcription():\n",
    "    \"\"\"Performs real-time speech-to-text transcription.\"\"\"\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    # Configure audio settings\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    streaming_config = speech.StreamingRecognitionConfig(config=config, interim_results=True)\n",
    "\n",
    "    def generator():\n",
    "        \"\"\"Generator that yields audio chunks from the microphone.\"\"\"\n",
    "        audio = pyaudio.PyAudio()\n",
    "        stream = audio.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=1024)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                yield speech.StreamingRecognizeRequest(audio_content=stream.read(1024))\n",
    "        except GeneratorExit:\n",
    "            stream.stop_stream()\n",
    "            stream.close()\n",
    "            audio.terminate()\n",
    "\n",
    "    # Perform streaming recognition\n",
    "    requests = generator()\n",
    "    responses = client.streaming_recognize(config=streaming_config, requests=requests)\n",
    "\n",
    "    # Print transcriptions\n",
    "    for response in responses:\n",
    "        for result in response.results:\n",
    "            if result.is_final:\n",
    "                print(f\"Transcription: {result.alternatives[0].transcript}\")\n",
    "\n",
    "# Call the function\n",
    "real_time_transcription()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording stopped.\n",
      "Transcribing audio...\n",
      "Transcribed text: say something British hello can you hear me\n",
      "Pronunciation mismatch:\n",
      "  Expected: ɛɡzampəl\n",
      "  Got: seɪ sʌmθɪŋ bɹɪtɪʃ hələʊ kan juː hiə miː\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from google.cloud import speech\n",
    "from phonemizer import phonemize\n",
    "\n",
    "# Configure PyAudio\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"user_input.wav\"\n",
    "\n",
    "def record_audio():\n",
    "    \"\"\"Record audio from the microphone.\"\"\"\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording stopped.\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save the recorded audio\n",
    "    with wave.open(WAVE_OUTPUT_FILENAME, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "def transcribe_audio():\n",
    "    \"\"\"Transcribe audio using Google Speech-to-Text.\"\"\"\n",
    "    client = speech.SpeechClient()\n",
    "    with open(WAVE_OUTPUT_FILENAME, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=RATE,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    if response.results:\n",
    "        return response.results[0].alternatives[0].transcript\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def compare_pronunciations(correct_word, user_input, language=\"en-gb\"):\n",
    "    \"\"\"Compare the pronunciation of a correct word with user input.\"\"\"\n",
    "    correct_pronunciation = phonemize(correct_word, language=language, backend=\"espeak\", strip=True)\n",
    "    user_pronunciation = phonemize(user_input, language=language, backend=\"espeak\", strip=True)\n",
    "\n",
    "    if correct_pronunciation == user_pronunciation:\n",
    "        print(\"Correct pronunciation!\")\n",
    "    else:\n",
    "        print(f\"Pronunciation mismatch:\\n\"\n",
    "              f\"  Expected: {correct_pronunciation}\\n\"\n",
    "              f\"  Got: {user_pronunciation}\")\n",
    "\n",
    "# Main flow\n",
    "if __name__ == \"__main__\":\n",
    "    record_audio()\n",
    "    print(\"Transcribing audio...\")\n",
    "    transcribed_text = transcribe_audio()\n",
    "    if transcribed_text:\n",
    "        print(f\"Transcribed text: {transcribed_text}\")\n",
    "        compare_pronunciations(\"example\", transcribed_text)\n",
    "    else:\n",
    "        print(\"No transcription available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynput import keyboard\n",
    "import pyaudio\n",
    "import wave\n",
    "import os\n",
    "import whisper\n",
    "\n",
    "# Configuration\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024\n",
    "OUTPUT_FOLDER = \"media\"\n",
    "OUTPUT_FILENAME = \"recorded_audio.wav\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "output_path = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open a stream for real-time audio capture\n",
    "stream = audio.open(format=FORMAT, \n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "\n",
    "print(\"Recording... Press 'S' to stop.\")\n",
    "\n",
    "frames = []\n",
    "recording = True\n",
    "\n",
    "# Function to handle key presses\n",
    "def on_press(key):\n",
    "    global recording\n",
    "    try:\n",
    "        if key.char == 's':  # Stop recording when 'S' is pressed\n",
    "            print(\"\\n'S' key pressed. Stopping recording...\")\n",
    "            recording = False\n",
    "            return False  # Stop the listener\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "# Start recording\n",
    "with keyboard.Listener(on_press=on_press) as listener:\n",
    "    while recording:\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    listener.join()\n",
    "\n",
    "# Stop and close the stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "# Save the recorded data to a WAV file\n",
    "with wave.open(output_path, 'wb') as wf:\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "\n",
    "print(f\"Audio file saved at: {output_path}\")\n",
    "\n",
    "# Load the Whisper model\n",
    "print(\"Loading Whisper model...\")\n",
    "model = whisper.load_model(\"base\")  # Use \"base\", \"small\", \"medium\", or \"large\"\n",
    "\n",
    "# Transcribe the audio\n",
    "print(\"Transcribing audio...\")\n",
    "result = model.transcribe(output_path)\n",
    "print(\"Transcription:\")\n",
    "print(result[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
