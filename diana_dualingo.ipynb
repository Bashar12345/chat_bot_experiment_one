{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accent Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open a stream for real-time audio capture\n",
    "stream = audio.open(format=pyaudio.paInt16, \n",
    "                    channels=1,\n",
    "                    rate=16000,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=1024)\n",
    "\n",
    "print(\"Recording...\")\n",
    "while True:\n",
    "    data = stream.read(1024)\n",
    "    # Process audio data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = \"your_openai_api_key\"\n",
    "\n",
    "# Transcribe audio using OpenAI API\n",
    "audio_file = open(\"media/recorded_audio.wav\", \"rb\")\n",
    "transcription = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "print(\"Transcription:\", transcription[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: \n",
      "Transcription: how much time\n",
      "Transcription:  hello can you hear me\n",
      "Transcription:  okay stop listening and give me the output\n",
      "Transcription:  it's perfect right\n",
      "Transcription:  so what do you think about it\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscription: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39malternatives[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtranscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mreal_time_transcription\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m, in \u001b[0;36mreal_time_transcription\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m responses \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mstreaming_recognize(config\u001b[38;5;241m=\u001b[39mstreaming_config, requests\u001b[38;5;241m=\u001b[39mrequests)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Print transcriptions\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_final\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:116\u001b[0m, in \u001b[0;36m_StreamingResponseIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stored_first_result\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# If the stream has already returned data, we cannot recover here.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/grpc/_channel.py:543\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/grpc/_channel.py:960\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_response_ready\u001b[39m():\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    956\u001b[0m         cygrpc\u001b[38;5;241m.\u001b[39mOperationType\u001b[38;5;241m.\u001b[39mreceive_message \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mdue\n\u001b[1;32m    957\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     )\n\u001b[0;32m--> 960\u001b[0m \u001b[43m_common\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_response_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/grpc/_common.py:156\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(wait_fn, wait_complete_fn, timeout, spin_cb)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait_complete_fn():\n\u001b[0;32m--> 156\u001b[0m         \u001b[43m_wait_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAXIMUM_WAIT_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspin_cb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m timeout\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/grpc/_common.py:116\u001b[0m, in \u001b[0;36m_wait_once\u001b[0;34m(wait_fn, timeout, spin_cb)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait_once\u001b[39m(\n\u001b[1;32m    112\u001b[0m     wait_fn: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mbool\u001b[39m],\n\u001b[1;32m    113\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    114\u001b[0m     spin_cb: Optional[Callable[[], \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[1;32m    115\u001b[0m ):\n\u001b[0;32m--> 116\u001b[0m     \u001b[43mwait_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spin_cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m         spin_cb()\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "from google.cloud import speech\n",
    "\n",
    "def real_time_transcription():\n",
    "    \"\"\"Performs real-time speech-to-text transcription.\"\"\"\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    # Configure audio settings\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    streaming_config = speech.StreamingRecognitionConfig(config=config, interim_results=True)\n",
    "\n",
    "    def generator():\n",
    "        \"\"\"Generator that yields audio chunks from the microphone.\"\"\"\n",
    "        audio = pyaudio.PyAudio()\n",
    "        stream = audio.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=1024)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                yield speech.StreamingRecognizeRequest(audio_content=stream.read(1024))\n",
    "        except GeneratorExit:\n",
    "            stream.stop_stream()\n",
    "            stream.close()\n",
    "            audio.terminate()\n",
    "\n",
    "    # Perform streaming recognition\n",
    "    requests = generator()\n",
    "    responses = client.streaming_recognize(config=streaming_config, requests=requests)\n",
    "\n",
    "    # Print transcriptions\n",
    "    for response in responses:\n",
    "        for result in response.results:\n",
    "            if result.is_final:\n",
    "                print(f\"Transcription: {result.alternatives[0].transcript}\")\n",
    "\n",
    "# Call the function\n",
    "real_time_transcription()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_route.c:878:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:878:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:878:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:878:(find_matching_chmap) Found no matching channel map\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_a52.c:1001:(_snd_pcm_a52_open) a52 is only for playback\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording stopped.\n",
      "Transcribing audio...\n",
      "No transcription available.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from google.cloud import speech\n",
    "from phonemizer import phonemize\n",
    "\n",
    "# Configure PyAudio\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"user_input.wav\"\n",
    "\n",
    "def record_audio():\n",
    "    \"\"\"Record audio from the microphone.\"\"\"\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording stopped.\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save the recorded audio\n",
    "    with wave.open(WAVE_OUTPUT_FILENAME, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "def transcribe_audio():\n",
    "    \"\"\"Transcribe audio using Google Speech-to-Text.\"\"\"\n",
    "    client = speech.SpeechClient()\n",
    "    with open(WAVE_OUTPUT_FILENAME, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=RATE,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    if response.results:\n",
    "        return response.results[0].alternatives[0].transcript\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def compare_pronunciations(correct_word, user_input, language=\"en-gb\"):\n",
    "    \"\"\"Compare the pronunciation of a correct word with user input.\"\"\"\n",
    "    correct_pronunciation = phonemize(correct_word, language=language, backend=\"espeak\", strip=True)\n",
    "    user_pronunciation = phonemize(user_input, language=language, backend=\"espeak\", strip=True)\n",
    "\n",
    "    if correct_pronunciation == user_pronunciation:\n",
    "        print(\"Correct pronunciation!\")\n",
    "    else:\n",
    "        print(f\"Pronunciation mismatch:\\n\"\n",
    "              f\"  Expected: {correct_pronunciation}\\n\"\n",
    "              f\"  Got: {user_pronunciation}\")\n",
    "\n",
    "# Main flow\n",
    "if __name__ == \"__main__\":\n",
    "    record_audio()\n",
    "    print(\"Transcribing audio...\")\n",
    "    transcribed_text = transcribe_audio()\n",
    "    if transcribed_text:\n",
    "        print(f\"Transcribed text: {transcribed_text}\")\n",
    "        compare_pronunciations(\"example\", transcribed_text)\n",
    "    else:\n",
    "        print(\"No transcription available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyttsx3 in ./myenv/lib/python3.12/site-packages (2.98)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting gtts\n",
      "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in ./myenv/lib/python3.12/site-packages (from gtts) (2.32.3)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in ./myenv/lib/python3.12/site-packages (from gtts) (8.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2.27->gtts) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2.27->gtts) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2.27->gtts) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2.27->gtts) (2024.12.14)\n",
      "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: gtts\n",
      "Successfully installed gtts-2.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install pyttsx3 if not already installed\n",
    "%pip install pyttsx3\n",
    "%pip install gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_route.c:878:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:878:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:878:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:878:(find_matching_chmap) Found no matching channel map\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_a52.c:1001:(_snd_pcm_a52_open) a52 is only for playback\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording stopped.\n",
      "Transcribing audio...\n",
      "No transcription available.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from google.cloud import speech\n",
    "from phonemizer import phonemize\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Configure PyAudio\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"user_input.wav\"\n",
    "TTS_OUTPUT_FILENAME = \"correct_pronunciation.mp3\"\n",
    "\n",
    "# Ensure mpg321 or another MP3 player is installed\n",
    "def play_audio(file_path):\n",
    "    \"\"\"Play audio file using an OS-specific command.\"\"\"\n",
    "    if shutil.which(\"mpg321\"):\n",
    "        os.system(f\"mpg321 {file_path}\")\n",
    "    elif shutil.which(\"ffplay\"):\n",
    "        os.system(f\"ffplay -nodisp -autoexit {file_path}\")\n",
    "    elif shutil.which(\"afplay\"):\n",
    "        os.system(f\"afplay {file_path}\")\n",
    "    else:\n",
    "        print(f\"Error: MP3 player not found. Install mpg321, ffplay, or afplay to play {file_path}.\")\n",
    "\n",
    "\n",
    "def record_audio():\n",
    "    \"\"\"Record audio from the microphone.\"\"\"\n",
    "    audio = pyaudio.PyAudio()\n",
    "    try:\n",
    "        stream = audio.open(format=FORMAT,\n",
    "                            channels=CHANNELS,\n",
    "                            rate=RATE,\n",
    "                            input=True,\n",
    "                            frames_per_buffer=CHUNK)\n",
    "\n",
    "        print(\"Recording...\")\n",
    "        frames = []\n",
    "        for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "\n",
    "        print(\"Recording stopped.\")\n",
    "\n",
    "        # Save the recorded audio\n",
    "        with wave.open(WAVE_OUTPUT_FILENAME, 'wb') as wf:\n",
    "            wf.setnchannels(CHANNELS)\n",
    "            wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "            wf.setframerate(RATE)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio.terminate()\n",
    "\n",
    "def transcribe_audio():\n",
    "    \"\"\"Transcribe audio using Google Speech-to-Text.\"\"\"\n",
    "    client = speech.SpeechClient()\n",
    "    with open(WAVE_OUTPUT_FILENAME, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=RATE,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    if response.results:\n",
    "        return response.results[0].alternatives[0].transcript\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def compare_pronunciation(correct, user):\n",
    "    \"\"\"Compare the pronunciation of the correct and user input.\"\"\"\n",
    "    errors = []\n",
    "    for c, u in zip(correct.split(), user.split()):\n",
    "        if c != u:\n",
    "            errors.append(f\"Pronounced '{u}' instead of '{c}'\")\n",
    "    return errors\n",
    "\n",
    "def text_to_speech(correct_pronunciation):\n",
    "    \"\"\"Use Google TTS to guide the user on correct pronunciation.\"\"\"\n",
    "    tts = gTTS(f\"Try pronouncing it as: {correct_pronunciation}\", lang=\"en\")\n",
    "    tts.save(TTS_OUTPUT_FILENAME)\n",
    "    play_audio(TTS_OUTPUT_FILENAME)\n",
    "\n",
    "# Main flow\n",
    "if __name__ == \"__main__\":\n",
    "    record_audio()\n",
    "    print(\"Transcribing audio...\")\n",
    "    transcribed_text = transcribe_audio()\n",
    "\n",
    "    if transcribed_text:\n",
    "        print(f\"Transcribed text: {transcribed_text}\")\n",
    "\n",
    "        correct_word = \"My name is khan\"\n",
    "        correct_pronunciation = phonemize(correct_word, language=\"en-gb\", backend=\"espeak\", strip=True)\n",
    "        user_pronunciation = phonemize(transcribed_text, language=\"en-gb\", backend=\"espeak\", strip=True)\n",
    "\n",
    "        # Compare phonemes\n",
    "        if correct_pronunciation == user_pronunciation:\n",
    "            print(\"Correct pronunciation!\")\n",
    "        else:\n",
    "            errors = compare_pronunciation(correct_word, transcribed_text)\n",
    "            print(\"Pronunciation mismatches:\")\n",
    "            for error in errors:\n",
    "                print(f\"  - {error}\")\n",
    "\n",
    "            # Guide the user with TTS\n",
    "            text_to_speech(correct_pronunciation)\n",
    "    else:\n",
    "        print(\"No transcription available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Steps\n",
    "### Step 1: Speech Recognition\n",
    "\n",
    "Use Whisper API to convert speech to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_route.c:878:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:878:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:878:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:878:(find_matching_chmap) Found no matching channel map\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_a52.c:1001:(_snd_pcm_a52_open) a52 is only for playback\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Press 'S' to stop.\n",
      "\n",
      "'S' key pressed. Stopping recording...\n",
      "Audio file saved at: media/recorded_audio.wav\n",
      "Loading Whisper model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vai/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vai/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:\n",
      " Hello, my name is Karen and I am from India.\n"
     ]
    }
   ],
   "source": [
    "from pynput import keyboard\n",
    "import pyaudio\n",
    "import wave\n",
    "import os\n",
    "import whisper\n",
    "\n",
    "# Configuration\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024\n",
    "OUTPUT_FOLDER = \"media\"\n",
    "OUTPUT_FILENAME = \"recorded_audio.wav\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "output_path = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open a stream for real-time audio capture\n",
    "stream = audio.open(format=FORMAT, \n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "\n",
    "print(\"Recording... Press 'S' to stop.\")\n",
    "\n",
    "frames = []\n",
    "recording = True\n",
    "\n",
    "# Function to handle key presses\n",
    "def on_press(key):\n",
    "    global recording\n",
    "    try:\n",
    "        if key.char == 's':  # Stop recording when 'S' is pressed\n",
    "            print(\"\\n'S' key pressed. Stopping recording...\")\n",
    "            recording = False\n",
    "            return False  # Stop the listener\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "# Start recording\n",
    "with keyboard.Listener(on_press=on_press) as listener:\n",
    "    while recording:\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    listener.join()\n",
    "\n",
    "# Stop and close the stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "# Save the recorded data to a WAV file\n",
    "with wave.open(output_path, 'wb') as wf:\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "\n",
    "print(f\"Audio file saved at: {output_path}\")\n",
    "\n",
    "# Load the Whisper model\n",
    "print(\"Loading Whisper model...\")\n",
    "model = whisper.load_model(\"base\")  # Use \"base\", \"small\", \"medium\", or \"large\"\n",
    "\n",
    "# Transcribe the audio\n",
    "print(\"Transcribing audio...\")\n",
    "result = model.transcribe(output_path)\n",
    "print(\"Transcription:\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect phonemes and linguistic features for accent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pronouncing\n",
      "  Downloading pronouncing-0.2.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmudict>=0.4.0 (from pronouncing)\n",
      "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: importlib-metadata>=5 in ./myenv/lib/python3.12/site-packages (from cmudict>=0.4.0->pronouncing) (8.5.0)\n",
      "Requirement already satisfied: importlib-resources>=5 in ./myenv/lib/python3.12/site-packages (from cmudict>=0.4.0->pronouncing) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.20 in ./myenv/lib/python3.12/site-packages (from importlib-metadata>=5->cmudict>=0.4.0->pronouncing) (3.21.0)\n",
      "Downloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m380.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pronouncing\n",
      "  Building wheel for pronouncing (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pronouncing: filename=pronouncing-0.2.0-py2.py3-none-any.whl size=6292 sha256=afdaebdb767b19befe7528f5bf8d34a92b76b77d0d5eed13bc77ea4ebc2d03fd\n",
      "  Stored in directory: /home/vai/.cache/pip/wheels/a0/76/15/dfdf38731993cdc4e86fd6d949c70c0e9786cf00073d8114d4\n",
      "Successfully built pronouncing\n",
      "Installing collected packages: cmudict, pronouncing\n",
      "Successfully installed cmudict-1.0.32 pronouncing-0.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pronouncing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HH AW1', 'AA1 R', 'Y UW1', 'D UW1 IH0 NG']\n"
     ]
    }
   ],
   "source": [
    "import pronouncing\n",
    "\n",
    "text = \"How are you doing today?\"\n",
    "words = text.split()\n",
    "phonemes = [pronouncing.phones_for_word(word)[0] for word in words if pronouncing.phones_for_word(word)]\n",
    "print(phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.0.10.2 Requires-Python >=3.6.0, <3.9; 0.0.10.3 Requires-Python >=3.6.0, <3.9; 0.0.11 Requires-Python >=3.6.0, <3.9; 0.0.12 Requires-Python >=3.6.0, <3.9; 0.0.13.1 Requires-Python >=3.6.0, <3.9; 0.0.13.2 Requires-Python >=3.6.0, <3.9; 0.0.14.1 Requires-Python >=3.6.0, <3.9; 0.0.15 Requires-Python >=3.6.0, <3.9; 0.0.15.1 Requires-Python >=3.6.0, <3.9; 0.0.9 Requires-Python >=3.6.0, <3.9; 0.0.9.1 Requires-Python >=3.6.0, <3.9; 0.0.9.2 Requires-Python >=3.6.0, <3.9; 0.0.9a10 Requires-Python >=3.6.0, <3.9; 0.0.9a9 Requires-Python >=3.6.0, <3.9; 0.1.0 Requires-Python >=3.6.0, <3.10; 0.1.1 Requires-Python >=3.6.0, <3.10; 0.1.2 Requires-Python >=3.6.0, <3.10; 0.1.3 Requires-Python >=3.6.0, <3.10; 0.10.0 Requires-Python >=3.7.0, <3.11; 0.10.1 Requires-Python >=3.7.0, <3.11; 0.10.2 Requires-Python >=3.7.0, <3.11; 0.11.0 Requires-Python >=3.7.0, <3.11; 0.11.1 Requires-Python >=3.7.0, <3.11; 0.12.0 Requires-Python >=3.7.0, <3.11; 0.13.0 Requires-Python >=3.7.0, <3.11; 0.13.1 Requires-Python >=3.7.0, <3.11; 0.13.2 Requires-Python >=3.7.0, <3.11; 0.13.3 Requires-Python >=3.7.0, <3.11; 0.14.0 Requires-Python >=3.7.0, <3.11; 0.14.2 Requires-Python >=3.7.0, <3.11; 0.14.3 Requires-Python >=3.7.0, <3.11; 0.15.0 Requires-Python >=3.9.0, <3.12; 0.15.1 Requires-Python >=3.9.0, <3.12; 0.15.2 Requires-Python >=3.9.0, <3.12; 0.15.4 Requires-Python >=3.9.0, <3.12; 0.15.5 Requires-Python >=3.9.0, <3.12; 0.15.6 Requires-Python >=3.9.0, <3.12; 0.16.0 Requires-Python >=3.9.0, <3.12; 0.16.1 Requires-Python >=3.9.0, <3.12; 0.16.3 Requires-Python >=3.9.0, <3.12; 0.16.4 Requires-Python >=3.9.0, <3.12; 0.16.5 Requires-Python >=3.9.0, <3.12; 0.16.6 Requires-Python >=3.9.0, <3.12; 0.17.0 Requires-Python >=3.9.0, <3.12; 0.17.1 Requires-Python >=3.9.0, <3.12; 0.17.2 Requires-Python >=3.9.0, <3.12; 0.17.4 Requires-Python >=3.9.0, <3.12; 0.17.5 Requires-Python >=3.9.0, <3.12; 0.17.6 Requires-Python >=3.9.0, <3.12; 0.17.7 Requires-Python >=3.9.0, <3.12; 0.17.8 Requires-Python >=3.9.0, <3.12; 0.17.9 Requires-Python >=3.9.0, <3.12; 0.18.0 Requires-Python >=3.9.0, <3.12; 0.18.1 Requires-Python >=3.9.0, <3.12; 0.18.2 Requires-Python >=3.9.0, <3.12; 0.19.0 Requires-Python >=3.9.0, <3.12; 0.19.1 Requires-Python >=3.9.0, <3.12; 0.2.0 Requires-Python >=3.6.0, <3.10; 0.2.1 Requires-Python >=3.6.0, <3.10; 0.2.2 Requires-Python >=3.6.0, <3.10; 0.20.0 Requires-Python >=3.9.0, <3.12; 0.20.1 Requires-Python >=3.9.0, <3.12; 0.20.2 Requires-Python >=3.9.0, <3.12; 0.20.3 Requires-Python >=3.9.0, <3.12; 0.20.4 Requires-Python >=3.9.0, <3.12; 0.20.5 Requires-Python >=3.9.0, <3.12; 0.20.6 Requires-Python >=3.9.0, <3.12; 0.21.0 Requires-Python >=3.9.0, <3.12; 0.21.1 Requires-Python >=3.9.0, <3.12; 0.21.2 Requires-Python >=3.9.0, <3.12; 0.21.3 Requires-Python >=3.9.0, <3.12; 0.22.0 Requires-Python >=3.9.0, <3.12; 0.3.0 Requires-Python >=3.6.0, <3.10; 0.3.1 Requires-Python >=3.6.0, <3.10; 0.4.0 Requires-Python >=3.6.0, <3.10; 0.4.1 Requires-Python >=3.6.0, <3.10; 0.4.2 Requires-Python >=3.6.0, <3.10; 0.5.0 Requires-Python >=3.6.0, <3.10; 0.6.0 Requires-Python >=3.6.0, <3.10; 0.6.1 Requires-Python >=3.6.0, <3.10; 0.6.2 Requires-Python >=3.6.0, <3.10; 0.7.0 Requires-Python >=3.7.0, <3.11; 0.7.1 Requires-Python >=3.7.0, <3.11; 0.8.0 Requires-Python >=3.7.0, <3.11; 0.9.0 Requires-Python >=3.7.0, <3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement TTS (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for TTS\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install TTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
    "import pronouncing\n",
    "\n",
    "# Extract phonemes using pronouncing\n",
    "def extract_phonemes(sentence):\n",
    "    words = sentence.split()\n",
    "    phonemes = [pronouncing.phones_for_word(word)[0] for word in words if pronouncing.phones_for_word(word)]\n",
    "    return phonemes\n",
    "\n",
    "# Load the Wav2Vec2 processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\",\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is running on: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model is running on: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "# Load processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\",\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# Set a reasonable model_max_length\n",
    "processor.tokenizer.model_max_length = 512  # Adjust as needed\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model is running on: {device}\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_audio(batch):\n",
    "    try:\n",
    "        # Step 1: Load audio and validate\n",
    "        audio = np.array(batch[\"audio\"][\"array\"], dtype=np.float32)\n",
    "        sampling_rate = batch[\"audio\"][\"sampling_rate\"]\n",
    "        print(f\"Audio Length: {len(audio)}, Sampling Rate: {sampling_rate}\")\n",
    "\n",
    "        # Step 2: Resample if necessary\n",
    "        if sampling_rate != 16000:\n",
    "            print(\"Resampling audio to 16kHz\")\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "            audio = resampler(torch.tensor(audio)).numpy()\n",
    "\n",
    "        # Step 3: Truncate or pad audio to a fixed length\n",
    "        max_audio_length = 16000\n",
    "        if len(audio) > max_audio_length:\n",
    "            print(f\"Truncating audio from {len(audio)} to {max_audio_length}\")\n",
    "            audio = audio[:max_audio_length]\n",
    "        else:\n",
    "            # Pad with zeros if shorter\n",
    "            audio = np.pad(audio, (0, max_audio_length - len(audio)), \"constant\")\n",
    "\n",
    "        # Normalize audio\n",
    "        audio = (audio - np.mean(audio)) / (np.std(audio) + 1e-7)\n",
    "\n",
    "        # Step 4: Convert text to tokens\n",
    "        phonemes = batch[\"sentence\"].split()  # Replace with phoneme logic if available\n",
    "        print(f\"Phonemes: {phonemes}\")\n",
    "\n",
    "        # Tokenize phonemes with additional debugging\n",
    "        try:\n",
    "            phoneme_ids = processor.tokenizer(\n",
    "                \" \".join(phonemes),\n",
    "                return_tensors=\"np\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=processor.tokenizer.model_max_length,\n",
    "            ).input_ids[0].astype(np.int32)  # Force smaller int32 type\n",
    "\n",
    "            print(f\"Phoneme IDs length: {len(phoneme_ids)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization: {e}\")\n",
    "            raise ValueError(f\"Tokenization failed for text: {' '.join(phonemes)}\")\n",
    "\n",
    "        # Step 5: Process audio for Wav2Vec2\n",
    "        input_values = processor(\n",
    "            audio,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"np\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_audio_length,\n",
    "        ).input_values[0]\n",
    "\n",
    "        # Debugging shapes\n",
    "        print(f\"Processed input_values shape: {input_values.shape}\")\n",
    "        print(f\"Processed phoneme_ids shape: {phoneme_ids.shape}\")\n",
    "\n",
    "        # Assign processed data to batch\n",
    "        batch[\"input_values\"] = input_values\n",
    "        batch[\"labels\"] = phoneme_ids\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        raise e\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Load dataset\n",
    "data = load_dataset(\"DTU54DL/common-accent\")\n",
    "\n",
    "# Preprocess dataset\n",
    "try:\n",
    "    data = data.map(preprocess_audio, remove_columns=[\"audio\", \"sentence\"], batched=False)\n",
    "except Exception as e:\n",
    "    print(f\"Dataset preprocessing failed: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Debug preprocessed data\n",
    "print(data[\"train\"][0])\n",
    "\n",
    "# Define metric computation\n",
    "def compute_metrics(pred):\n",
    "    predictions = torch.argmax(torch.tensor(pred.predictions), axis=-1)\n",
    "    label_ids = torch.tensor(pred.label_ids)\n",
    "    accuracy = (predictions == label_ids).float().mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=processor)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2_common_accent_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"test\"],\n",
    "    tokenizer=processor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(\"./wav2vec2_common_accent_finetuned\")\n",
    "processor.save_pretrained(\"./wav2vec2_common_accent_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'common_voice_en_101616.mp3', 'array': array([-3.78349796e-10, -3.49245965e-10, -2.61934474e-10, ...,\n",
      "       -1.45020604e-05,  5.48214593e-05,  4.17054107e-05]), 'sampling_rate': 16000}, 'sentence': 'Men in orange vests are at work on a construction site.', 'accent': 'India and South Asia (India, Pakistan, Sri Lanka)'}\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2CTCTokenizer(name_or_path='facebook/wav2vec2-base-960h', vocab_size=32, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
      "\t1: AddedToken(\"<s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
      "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(processor.tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from TTS.tts.utils.text.phonemizers import get_phonemizer_by_name\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "# Initialize ConquiTTS phonemizer\n",
    "phonemizer = get_phonemizer_by_name(name=\"espeak\", language=\"en-us\", phoneme_cache_path=\"./phoneme_cache\")\n",
    "phonemizer.set_gpu(True)  # Enable GPU support for phonemizer\n",
    "\n",
    "# Load Wav2Vec2 processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\",\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# Set a reasonable model_max_length\n",
    "processor.tokenizer.model_max_length = 512  # Cap tokenized length\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model is running on: {device}\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_audio(batch):\n",
    "    try:\n",
    "        # Step 1: Load audio and validate\n",
    "        audio = np.array(batch[\"audio\"][\"array\"], dtype=np.float32)\n",
    "        sampling_rate = batch[\"audio\"][\"sampling_rate\"]\n",
    "        print(f\"Audio Length: {len(audio)}, Sampling Rate: {sampling_rate}\")\n",
    "\n",
    "        # Step 2: Resample if necessary\n",
    "        if sampling_rate != 16000:\n",
    "            print(\"Resampling audio to 16kHz\")\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "            audio = resampler(torch.tensor(audio)).numpy()\n",
    "\n",
    "        # Step 3: Truncate or pad audio to a fixed length\n",
    "        max_audio_length = 16000\n",
    "        if len(audio) > max_audio_length:\n",
    "            print(f\"Truncating audio from {len(audio)} to {max_audio_length}\")\n",
    "            audio = audio[:max_audio_length]\n",
    "        else:\n",
    "            # Pad with zeros if shorter\n",
    "            audio = np.pad(audio, (0, max_audio_length - len(audio)), \"constant\")\n",
    "\n",
    "        # Normalize audio\n",
    "        audio = (audio - np.mean(audio)) / (np.std(audio) + 1e-7)\n",
    "\n",
    "        # Step 4: Convert text to phonemes using ConquiTTS\n",
    "        phonemes = phonemizer.phonemize(batch[\"sentence\"])\n",
    "        print(f\"Phonemes: {phonemes}\")\n",
    "\n",
    "        # Tokenize phonemes\n",
    "        phoneme_ids = processor.tokenizer(\n",
    "            phonemes,\n",
    "            return_tensors=\"np\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=processor.tokenizer.model_max_length,\n",
    "        ).input_ids[0].astype(np.int32)  # Force smaller int32 type\n",
    "\n",
    "        # Step 5: Process audio for Wav2Vec2\n",
    "        input_values = processor(\n",
    "            audio,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"np\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_audio_length,\n",
    "        ).input_values[0]\n",
    "\n",
    "        # Debugging shapes\n",
    "        print(f\"Processed input_values shape: {input_values.shape}\")\n",
    "        print(f\"Processed phoneme_ids shape: {phoneme_ids.shape}\")\n",
    "\n",
    "        # Assign processed data to batch\n",
    "        batch[\"input_values\"] = input_values\n",
    "        batch[\"labels\"] = phoneme_ids\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        raise e\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Load dataset\n",
    "data = load_dataset(\"DTU54DL/common-accent\")\n",
    "\n",
    "# Preprocess dataset\n",
    "try:\n",
    "    data = data.map(preprocess_audio, remove_columns=[\"audio\", \"sentence\"], batched=False)\n",
    "except Exception as e:\n",
    "    print(f\"Dataset preprocessing failed: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Debug preprocessed data\n",
    "print(data[\"train\"][0])\n",
    "\n",
    "# Define metric computation\n",
    "def compute_metrics(pred):\n",
    "    predictions = torch.argmax(torch.tensor(pred.predictions), axis=-1)\n",
    "    label_ids = torch.tensor(pred.label_ids)\n",
    "    accuracy = (predictions == label_ids).float().mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=processor)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2_common_accent_finetuned\",\n",
    "    run_name=\"wav2vec2_finetuning_run_1\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"test\"],\n",
    "    tokenizer=processor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CMUARCTIC', 'CMUDict', 'COMMONVOICE', 'DR_VCTK', 'FluentSpeechCommands', 'GTZAN', 'IEMOCAP', 'LIBRISPEECH', 'LIBRITTS', 'LJSPEECH', 'LibriLightLimited', 'LibriMix', 'LibriSpeechBiasing', 'MUSDB_HQ', 'QUESST14', 'SPEECHCOMMANDS', 'Snips', 'TEDLIUM', 'VCTK_092', 'VoxCeleb1Identification', 'VoxCeleb1Verification', 'YESNO', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'cmuarctic', 'cmudict', 'commonvoice', 'dr_vctk', 'fluentcommands', 'gtzan', 'iemocap', 'librilight_limited', 'librimix', 'librispeech', 'librispeech_biasing', 'libritts', 'ljspeech', 'musdb_hq', 'quesst14', 'snips', 'speechcommands', 'tedlium', 'utils', 'vctk', 'voxceleb1', 'yesno']\n"
     ]
    }
   ],
   "source": [
    "import torchaudio.datasets\n",
    "print(dir(torchaudio.datasets))  # Check if \"TIMIT\" is listed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/train-clean-100.tar.gz.5046f40e94624328bc9fe068592a29d0.partial'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the LIBRISPEECH dataset\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m librispeech_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIBRISPEECH\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain-clean-100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Subset to load: \"train-clean-100\", \"test-clean\", etc.\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Download the dataset if not already available\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Inspect a single sample\u001b[39;00m\n\u001b[1;32m     11\u001b[0m waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id \u001b[38;5;241m=\u001b[39m librispeech_train[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/torchaudio/datasets/librispeech.py:113\u001b[0m, in \u001b[0;36mLIBRISPEECH.__init__\u001b[0;34m(self, root, url, folder_in_archive, download)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m--> 113\u001b[0m         \u001b[43m_download_librispeech\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    116\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please set `download=True` to download the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/torchaudio/datasets/librispeech.py:42\u001b[0m, in \u001b[0;36m_download_librispeech\u001b[0;34m(root, url)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(archive):\n\u001b[1;32m     41\u001b[0m     checksum \u001b[38;5;241m=\u001b[39m _CHECKSUMS\u001b[38;5;241m.\u001b[39mget(download_url, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m _extract_tar(archive)\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/myenv/lib/python3.12/site-packages/torch/hub.py:726\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    724\u001b[0m tmp_dst \u001b[38;5;241m=\u001b[39m dst \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.partial\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 726\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtmp_dst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/train-clean-100.tar.gz.5046f40e94624328bc9fe068592a29d0.partial'"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "# Load the LIBRISPEECH dataset\n",
    "librispeech_train = torchaudio.datasets.LIBRISPEECH(\n",
    "    root=\"./data\",\n",
    "    url=\"train-clean-100\",  # Subset to load: \"train-clean-100\", \"test-clean\", etc.\n",
    "    download=True,          # Download the dataset if not already available\n",
    ")\n",
    "\n",
    "# Inspect a single sample\n",
    "waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = librispeech_train[0]\n",
    "\n",
    "print(\"Waveform shape:\", waveform.shape)  # Shape: [number_of_samples]\n",
    "print(\"Sample rate:\", sample_rate)        # Typically 16,000 Hz\n",
    "print(\"Transcript:\", transcript)\n",
    "print(\"Speaker ID:\", speaker_id)\n",
    "print(\"Chapter ID:\", chapter_id)\n",
    "print(\"Utterance ID:\", utterance_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
