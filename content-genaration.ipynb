{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content generation (image, video, audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai/whisper-large-v3-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the transformers package\n",
    "%pip install transformers\n",
    "%pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install librosa soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/home/vai/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  but the same time safe to scarlet and to a silence which had been friends to that the doctor was put into her thoughts\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the pipeline with the Whisper model\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3-turbo\")\n",
    "\n",
    "# Specify your audio file path (wav, mp3, etc.)\n",
    "audio_file_path = \"media/sample-2.mp3\"\n",
    "\n",
    "# Run the speech recognition pipeline\n",
    "result = pipe(audio_file_path)\n",
    "\n",
    "# Output the transcription\n",
    "print(\"Transcription:\", result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(audio_file_path, language=\"bn\")  # for Bengali\n",
    "# or\n",
    "result = pipe(audio_file_path, language=\"en\")  # for English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in ./venv/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in ./venv/lib/python3.10/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# Extract audio from the dataset and get the sample rate\n",
    "audio_array = ds[0][\"audio\"][\"array\"]\n",
    "sample_rate = ds[0][\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "# Ensure the waveform is at the required sample rate for Whisper (16 kHz)\n",
    "if sample_rate != 16000:\n",
    "    # Optionally resample here if necessary\n",
    "    import torchaudio\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    audio_array = resampler(torch.tensor(audio_array).unsqueeze(0)).squeeze(0).numpy()\n",
    "    sample_rate = 16000  # Update sample rate after resampling\n",
    "\n",
    "# Process the audio with the correct sample rate\n",
    "inputs = processor(audio_array, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "input_features = inputs.input_features\n",
    "\n",
    "# Generate transcription\n",
    "generated_ids = model.generate(input_features)\n",
    "\n",
    "# Decode the generated ids into text\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# Output the transcription\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facebook/wav2vec2-large-960h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: BUT THE SAME TIME SAFE TO SCARLOT ANTO A SILENCE WHICH HAD BEEN FRIENDS TO THAT THE DOCTOR WAS PUT INTO HER THOUGHTS\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the pipeline with the Wav2Vec2 model\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Specify your audio file path (wav, mp3, etc.)\n",
    "audio_file_path = \"media/sample-2.mp3\"  # Make sure this is a wav file\n",
    "\n",
    "# Run the speech recognition pipeline\n",
    "result = pipe(audio_file_path)\n",
    "\n",
    "# Output the transcription\n",
    "print(\"Transcription:\", result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facebook/wav2vec2-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/wav2vec2-base\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
