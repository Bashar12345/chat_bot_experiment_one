{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers -U\n",
    "%pip install sentencepiece\n",
    "%pip install Pillow\n",
    "%pip install torch\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install tqdm\n",
    "%pip install torchtext\n",
    "%pip install torchsummary\n",
    "%pip install torchviz\n",
    "%pip install tensorboard\n",
    "%pip install tensorboardX\n",
    "%pip install torchmetrics\n",
    "%pip install pytorch-lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to securely input the Hugging Face API token\n",
    "api_token = getpass(\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### microsoft/layoutlmv2-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Set up pipeline with API key\n",
    "generator = pipeline('text-generation',\n",
    "                     api_key='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')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Concepts in the Code\n",
    "###### OCR with Pytesseract:\n",
    "Extracts words and their positions from the image.\n",
    "\n",
    "###### Bounding Box Normalization:\n",
    "Ensures coordinates match LayoutLMv2’s expected input range.\n",
    "\n",
    "###### Tokenization and Encoding:\n",
    "Prepares the image and text for input into the transformer model.\n",
    "\n",
    "###### Answer Extraction:\n",
    "Identifies the most probable span of text corresponding to the answer.\n",
    "\n",
    "###### Pretrained Model:\n",
    "Leverages LayoutLMv2’s capabilities to process structured documents and handle layout-aware questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv2ForQuestionAnswering, LayoutLMv2Processor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = LayoutLMv2ForQuestionAnswering.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "\n",
    "# Load the document image\n",
    "image_path = \"/home/vai/Desktop/chat_bot_experiment_one/cloud/test.png\"  # Replace with your document image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# Define the question and context (text from the document)\n",
    "question = \"waht is langChan format ?\"\n",
    "\n",
    "\n",
    "# Define some example text and bounding boxes\n",
    "text = [\"What\", \"is\", \"LangChain\", \"format\", \"?\"]\n",
    "bbox = [[50, 50, 150, 100], [160, 50, 240, 100], [250, 50, 350, 100], [360, 50, 460, 100], [470, 50, 520, 100]]\n",
    "\n",
    "# Process the inputs (image, text, and bounding boxes)\n",
    "encoded_inputs = processor(image, text,return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "outputs = model(**encoded_inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Extract the start and end logits for the answer\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Get the answer tokens\n",
    "start_index = torch.argmax(start_logits, dim=1).item()\n",
    "end_index = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "# Decode the answer\n",
    "tokens = encoded_inputs[\"input_ids\"].squeeze()\n",
    "answer = processor.tokenizer.decode(tokens[start_index:end_index + 1])\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fine-tunned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv2Processor, LayoutLMv2ForQuestionAnswering\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytesseract\n",
    "\n",
    "# Load model and processor\n",
    "model = LayoutLMv2ForQuestionAnswering.from_pretrained(\"tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\")\n",
    "processor = LayoutLMv2Processor.from_pretrained(\"tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\")\n",
    "\n",
    "# Load image\n",
    "image_path = \"/home/vai/Desktop/chat_bot_experiment_one/cloud/test.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Extract words and bounding boxes using OCR\n",
    "ocr_results = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "words = []\n",
    "boxes = []\n",
    "\n",
    "# Get image dimensions\n",
    "image_width, image_height = image.size\n",
    "\n",
    "for i in range(len(ocr_results['text'])):\n",
    "    if ocr_results['text'][i].strip():  # Ignore empty text\n",
    "        words.append(ocr_results['text'][i])\n",
    "        \n",
    "        # Extract bounding box coordinates\n",
    "        x, y, w, h = (ocr_results['left'][i], ocr_results['top'][i],\n",
    "                      ocr_results['width'][i], ocr_results['height'][i])\n",
    "\n",
    "        # Normalize coordinates to 0–1000 range\n",
    "        normalized_bbox = [\n",
    "            int(1000 * (x / image_width)),\n",
    "            int(1000 * (y / image_height)),\n",
    "            int(1000 * ((x + w) / image_width)),\n",
    "            int(1000 * ((y + h) / image_height))\n",
    "        ]\n",
    "        boxes.append(normalized_bbox)\n",
    "\n",
    "\n",
    "# Question\n",
    "question = \"Tell me someting about covid?\"\n",
    "\n",
    "# Encode inputs with normalized bounding boxes\n",
    "encoded_inputs = processor(\n",
    "    image,\n",
    "    words,\n",
    "    boxes=boxes,  # Normalized bounding boxes\n",
    "    return_tensors=\"pt\",\n",
    "    # truncation=True,\n",
    "    \n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "outputs = model(**encoded_inputs)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Extract the answer\n",
    "start_index = torch.argmax(start_logits, dim=1).item()\n",
    "end_index = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "tokens = encoded_inputs[\"input_ids\"].squeeze()\n",
    "answer = processor.tokenizer.decode(tokens[start_index:end_index + 1])\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### check quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[Document', '(page_content=\"This', 'transcript', 'is', 'provided', 'for', 'the', 'convenience', 'of', 'investors', 'only,', 'for', 'a', 'full', 'recording', 'pleas', 'e', 'see', 'the', 'Q4', '2021', 'Earnings', 'Call', 'webcast', '.\\\\n\\\\nAlphabet', 'Q4', '2021', 'Earnings', 'Call', 'February', '1,', '2022\\\\n\\\\nOperator:', 'Welcome', 'eve', 'ryone.', 'And', 'thank', 'you', 'for', 'standing', 'by', 'for', 'the', 'Alphabet', 'fourth', 'quarter', '2021', 'earnings', 'conference', 'call.', 'At', 'this', 'time,', 'all', 'participants', 'are', 'in', 'a', 'listen-only', 'mode.', 'After', 'the', 'speaker', 'presentation,', 'there', 'will', 'be', 'a', 'question', 'and', 'answer', 'session.', 'To', 'ask', 'a', 'question', 'during', 'the', 'session,', 'you', 'will', 'need', 'to', 'press', 'star', 'one', 'on', 'your', 'telephone.', 'If', 'you', 'require', 'any', 'further', 'a', 'ssistance,', 'please', 'press', 'star', 'zero.', 'I', 'would', 'now', 'like', 'to', 'hand', 'the', 'conference', 'over', 'to', 'your', 'speaker', 'today,', 'Jim', 'Friedland,', 'Director', 'of', 'Investor', 'Relations.', 'Please', 'go', 'ahead.\\\\n\\\\nJim', 'Friedland,', 'Director', 'Investor', 'Relations:', 'Thank', 'you.', 'Good', 'after', 'noon,', 'everyone,', 'and', 'welcome', 'to', 'Alphabet’s', 'fourth', 'quarter', '2021', 'earnings', 'conference', 'call.', 'With', 'us', 'today', 'are', 'Sundar', 'Pich', 'ai,', 'Philipp', 'Schindler', 'and', 'Ruth', 'Porat.', 'Now', 'I‘1ll', 'quickly', 'cover', 'the', 'Safe', 'Harbor.', 'Some', 'of', 'the', 'statements', 'that', 'we', 'make', 'tod', 'ay', 'regarding', 'our', 'business,', 'operations,', 'and', 'financial', 'performance,', 'including', 'the', 'effect', 'of', 'the', 'COVID-19', 'pandemic', 'on', 'th', 'ose', 'areas,', 'may', 'be', 'considered', 'forward-looking,', 'and', 'such', 'statements', 'involve', 'a', 'number', 'of', 'risks', 'and', 'uncertainties', 'that', 'co', 'uld', 'cause', 'actual', 'results', 'to', 'differ', 'materially.', 'For', 'more', 'information,', 'please', 'refer', 'to', 'the', 'risk', 'factors', 'discussed', 'in', 'ou', 'x', 'Forms', '10-K', 'and', '10-Q', 'filed', 'with', 'the', 'SEC,', 'including', 'our', 'upcoming', 'Form', '10-K', 'filing', 'for', 'the', 'year', 'ended', 'December', '31,', '202', '1.', 'During', 'this', 'call,', 'we', 'will', 'present', 'both', 'GAAP', 'and', 'non-GAAP', 'financial', 'measures.', 'A', 'reconciliation', 'of', 'non-GAAP', 'to', 'GAAP', 'measures', 'is', 'included', 'in', \"today's\", 'press', 'release,', 'which', 'is', 'distributed', 'and', 'available', 'to', 'the', 'public', 'through', 'our', 'Investor', 'Relations', 'website', 'located', 'at', 'abc.xyz/investor.', 'And', 'now', 'I‘1l', 'turn', 'the', 'call', 'over', 'to', 'Sundar.\\\\n\\\\nSundar', 'Pichai,', 'CEO', 'Alpha', 'bet', 'and', 'Google:', 'Thank', 'you,', 'Jim,', 'and', 'Happy', 'New', 'Year,', 'everyone.', 'The', 'last', 'few', 'months', 'have', 'been', 'challenging', 'for', 'communiti', 'es', 'everywhere', 'because', 'of', 'Omicron.', 'I’m', 'grateful', 'for', 'the', 'frontline', 'healthcare', 'workers', 'who', 'are', 'helping', 'us', 'through', 'it,', 'an', 'd', 'glad', 'to', 'see', 'signs', 'that', 'this', 'wave', 'is', 'receding', 'in', 'many', 'parts', 'of', 'the', 'world.', 'Whether', 'it’s', 'helping', 'people', 'find', 'a', 'COVID', 't', 'esting', 'center,', 'learn', 'a', 'new', 'skill,', 'or', 'launch', 'a', 'new', 'business,', 'our', 'mission', 'to', 'organize', 'the', 'world’s', 'information', 'and', 'make', 'it', 'universally', 'accessible', 'and', 'useful', 'is', 'as', 'relevant', 'today', 'as', 'it’s', 'ever', 'been.\\\\n\\\\nIn', '2022,', \"we'll\", 'stay', 'focused', 'on', 'evolvi', 'ng', 'our', 'knowledge', 'and', 'information', 'products,', 'including', 'Search,', 'Maps,', 'and', 'YouTube,', 'to', 'be', 'even', 'more', 'helpful.', 'Investments', 'in', 'AI', 'will', 'be', 'key,', 'and', 'we’ll', 'continue', 'to', 'make', 'improvements', 'to', 'conversational', 'interfaces', 'like', 'the', 'Assistant.', 'I’1l', 'begi', 'n', 'by', 'touching', 'on', 'a', 'few', 'highlights', 'from', 'Q4.\\\\n\\\\nOur', 'new', 'AI', 'models', 'are', 'helping', 'to', 'create', 'information', 'experiences', 'that', 'ar', 'e', 'truly', 'conversational,', 'multimodal,', 'and', 'personal.', 'For', 'example,', 'Multitask', 'Unified', 'Model', '--', 'or', 'MUM', 'for', 'short', '--', 'has', 'imp', 'roved', 'searches', 'for', 'vaccine', 'information.', 'And', 'soon,', \"we'll\", 'introduce', 'new', 'ways', 'to', 'search', 'with', 'images', 'and', 'words', 'simultaneo', 'usly.', 'In', 'October,', 'we', 'introduced', 'a', 'new', 'AI', 'architecture,', 'called', 'Pathways.', 'AI', 'models', 'are', 'typically', 'trained', 'to', 'do', 'only', 'on', 'e', 'thing.', 'With', 'Pathways', 'a', 'single', 'model', 'can', 'be', 'trained', 'to', 'do', 'thousands,', 'even', 'millions,', 'of', 'things.\\\\n\\\\nFrom', 'MUM', 'to', 'Pathwa', 'ys,', 'to', 'BERT', 'and', 'more,', 'these', 'deep', 'AI', 'investments', 'are', 'helping', 'us', 'lead', 'in', 'search', 'quality.', 'They’re', 'also', 'powering', 'innovati']\n",
      "[[4, 12, 77, 34], [81, 9, 238, 35], [248, 9, 331, 34], [342, 9, 357, 28], [366, 9, 434, 34], [443, 10, 467, 28], [477, 10, 501, 28], [510, 9, 603, 28], [612, 10, 627, 28], [638, 9, 712, 28], [722, 10, 761, 34], [774, 10, 798, 28], [807, 15, 815, 28], [824, 10, 857, 28], [866, 9, 942, 34], [951, 10, 991, 34], [2, 49, 10, 62], [20, 49, 43, 62], [53, 45, 77, 62], [87, 45, 102, 66], [113, 45, 144, 62], [155, 43, 221, 68], [231, 45, 263, 62], [273, 45, 331, 62], [344, 42, 450, 68], [460, 45, 475, 66], [486, 45, 517, 62], [528, 43, 594, 68], [604, 45, 636, 62], [646, 45, 713, 68], [723, 45, 736, 66], [748, 42, 888, 68], [899, 45, 958, 62], [968, 49, 992, 62], [3, 83, 50, 102], [61, 79, 86, 96], [96, 79, 137, 96], [146, 83, 171, 102], [181, 79, 205, 96], [214, 77, 281, 102], [290, 79, 307, 102], [316, 79, 340, 96], [350, 79, 374, 96], [383, 79, 450, 102], [460, 79, 510, 96], [519, 80, 577, 102], [587, 79, 619, 96], [629, 77, 695, 102], [706, 79, 789, 96], [799, 79, 837, 96], [848, 80, 865, 96], [875, 77, 907, 96], [917, 77, 956, 101], [968, 79, 992, 96], [2, 112, 103, 136], [113, 117, 137, 131], [147, 112, 162, 131], [172, 117, 179, 131], [189, 112, 281, 136], [290, 113, 329, 131], [341, 113, 382, 131], [392, 113, 416, 131], [427, 113, 484, 136], [493, 112, 600, 136], [612, 113, 653, 131], [662, 112, 695, 131], [705, 113, 721, 131], [731, 117, 739, 131], [747, 112, 815, 136], [824, 113, 848, 131], [858, 117, 907, 131], [918, 112, 982, 131], [2, 148, 18, 165], [28, 147, 52, 165], [62, 151, 69, 165], [78, 146, 145, 170], [155, 146, 205, 170], [214, 147, 239, 165], [249, 146, 312, 169], [324, 151, 348, 170], [357, 146, 390, 165], [401, 146, 434, 165], [443, 148, 459, 165], [468, 151, 509, 170], [519, 148, 552, 165], [561, 151, 585, 165], [595, 151, 611, 165], [620, 151, 653, 170], [663, 146, 744, 170], [757, 147, 771, 165], [781, 151, 806, 170], [815, 146, 874, 170], [883, 151, 907, 170], [918, 146, 975, 165], [985, 151, 992, 165], [3, 180, 84, 203], [96, 180, 145, 204], [155, 185, 195, 204], [206, 183, 239, 199], [249, 185, 287, 199], [300, 183, 306, 199], [315, 180, 357, 199], [367, 185, 392, 199], [401, 180, 433, 199], [442, 181, 459, 199], [468, 180, 501, 199], [510, 180, 535, 199], [544, 180, 628, 199], [638, 185, 671, 199], [680, 183, 696, 199], [705, 185, 738, 204], [748, 180, 806, 204], [815, 180, 863, 204], [875, 180, 900, 199], [909, 180, 990, 203], [2, 214, 69, 233], [78, 214, 94, 233], [105, 215, 171, 233], [180, 214, 262, 233], [274, 214, 323, 233], [333, 219, 348, 239], [359, 213, 468, 234], [477, 214, 558, 237], [570, 214, 636, 233], [646, 215, 712, 233], [722, 214, 804, 233], [815, 214, 857, 233], [866, 219, 896, 239], [908, 214, 942, 233], [951, 214, 992, 233], [2, 254, 41, 271], [53, 254, 126, 273], [138, 248, 162, 267], [171, 248, 230, 267], [240, 250, 256, 267], [265, 248, 348, 273], [359, 248, 408, 267], [417, 250, 475, 273], [486, 250, 517, 267], [528, 248, 594, 273], [604, 248, 687, 267], [697, 248, 736, 267], [747, 248, 780, 267], [790, 254, 805, 267], [815, 248, 857, 273], [866, 254, 890, 267], [901, 248, 950, 267], [960, 248, 992, 267], [3, 282, 24, 306], [37, 282, 94, 307], [105, 282, 179, 301], [189, 282, 213, 301], [222, 282, 256, 301], [265, 284, 312, 301], [324, 285, 349, 301], [359, 282, 390, 301], [401, 282, 459, 307], [468, 288, 509, 301], [519, 282, 543, 301], [553, 282, 585, 301], [595, 282, 651, 301], [664, 284, 696, 301], [706, 282, 721, 301], [731, 282, 755, 301], [765, 284, 848, 301], [858, 282, 890, 301], [899, 288, 916, 301], [925, 282, 958, 301], [968, 282, 993, 301], [3, 322, 18, 341], [28, 316, 103, 341], [112, 322, 137, 336], [146, 316, 219, 340], [231, 316, 321, 341], [333, 316, 357, 336], [368, 316, 441, 336], [451, 316, 550, 341], [562, 316, 636, 341], [646, 316, 671, 336], [680, 316, 730, 336], [739, 316, 754, 336], [765, 316, 789, 336], [798, 316, 864, 336], [875, 316, 941, 341], [951, 322, 967, 336], [976, 316, 992, 336], [2, 356, 26, 370], [37, 356, 84, 374], [95, 356, 120, 375], [129, 351, 145, 370], [155, 351, 239, 370], [249, 351, 380, 375], [392, 351, 416, 370], [427, 351, 459, 370], [469, 352, 551, 370], [562, 351, 620, 370], [629, 356, 637, 370], [646, 351, 696, 370], [706, 351, 721, 370], [731, 351, 771, 370], [782, 351, 807, 370], [815, 351, 924, 370], [934, 351, 967, 370], [976, 356, 992, 370], [2, 385, 27, 404], [37, 390, 77, 404], [87, 385, 136, 404], [146, 385, 204, 404], [214, 386, 230, 404], [240, 383, 289, 404], [298, 385, 389, 409], [401, 386, 425, 404], [434, 390, 467, 404], [478, 383, 575, 408], [587, 385, 636, 409], [646, 385, 687, 404], [697, 386, 712, 404], [722, 385, 747, 404], [756, 383, 789, 404], [799, 385, 856, 404], [866, 385, 942, 404], [951, 383, 967, 404], [976, 390, 992, 404], [2, 424, 10, 438], [19, 420, 60, 438], [71, 419, 103, 438], [113, 419, 137, 438], [148, 419, 179, 441], [189, 418, 230, 438], [239, 418, 272, 438], [282, 419, 306, 438], [316, 420, 346, 442], [359, 418, 434, 442], [443, 424, 467, 438], [477, 419, 543, 443], [553, 420, 587, 438], [596, 419, 628, 438], [638, 418, 688, 442], [697, 419, 721, 438], [731, 419, 755, 438], [765, 424, 798, 443], [807, 419, 848, 438], [858, 419, 925, 438], [935, 419, 956, 442], [969, 419, 991, 438], [4, 453, 16, 471], [28, 452, 78, 476], [87, 452, 119, 472], [129, 453, 168, 476], [179, 459, 196, 472], [205, 452, 238, 472], [248, 454, 306, 476], [315, 453, 348, 472], [358, 454, 391, 472], [401, 453, 425, 472], [434, 454, 501, 472], [511, 452, 585, 472], [594, 459, 668, 472], [679, 454, 688, 472], [697, 452, 815, 472], [824, 453, 839, 472], [850, 454, 916, 472], [925, 454, 941, 472], [951, 454, 984, 472], [2, 493, 68, 506], [79, 486, 94, 506], [105, 486, 171, 506], [181, 486, 196, 505], [206, 486, 263, 510], [274, 493, 314, 510], [324, 487, 389, 509], [400, 486, 442, 506], [452, 486, 467, 506], [477, 486, 569, 506], [578, 487, 603, 506], [613, 486, 687, 506], [697, 489, 712, 506], [722, 487, 747, 506], [756, 486, 806, 510], [815, 487, 874, 510], [883, 493, 907, 506], [918, 489, 984, 506], [2, 520, 77, 540], [87, 520, 145, 540], [155, 521, 213, 540], [223, 523, 239, 540], [248, 520, 389, 545], [400, 521, 425, 539], [434, 527, 459, 540], [469, 521, 501, 539], [510, 523, 543, 539], [553, 521, 577, 540], [587, 521, 619, 540], [629, 527, 662, 540], [671, 523, 688, 540], [697, 520, 839, 540], [849, 520, 905, 545], [917, 523, 942, 540], [950, 521, 993, 545], [2, 556, 26, 573], [37, 556, 61, 573], [70, 556, 126, 579], [138, 556, 179, 573], [188, 561, 219, 579], [231, 554, 262, 577], [274, 556, 298, 573], [307, 557, 348, 579], [358, 557, 383, 575], [392, 557, 431, 579], [443, 561, 516, 579], [528, 556, 552, 575], [562, 556, 594, 573], [605, 556, 629, 575], [637, 556, 687, 573], [697, 556, 730, 575], [739, 556, 772, 575], [782, 554, 874, 579], [884, 556, 907, 573], [917, 554, 992, 575], [2, 595, 17, 607], [28, 590, 111, 613], [121, 590, 179, 607], [188, 590, 204, 607], [214, 588, 278, 607], [291, 591, 315, 607], [324, 590, 390, 613], [401, 590, 425, 607], [434, 590, 458, 607], [469, 588, 543, 607], [553, 590, 636, 607], [645, 590, 703, 609], [713, 590, 738, 609], [748, 595, 772, 607], [782, 588, 840, 613], [849, 595, 864, 607], [875, 590, 933, 613], [943, 588, 964, 612], [977, 595, 992, 607], [2, 624, 10, 642], [19, 624, 52, 647], [62, 625, 78, 642], [87, 628, 111, 642], [122, 622, 162, 647], [172, 624, 204, 642], [214, 622, 246, 642], [256, 628, 289, 643], [300, 622, 314, 642], [324, 622, 391, 647], [401, 622, 416, 642], [425, 629, 459, 647], [468, 625, 508, 647], [519, 624, 535, 642], [544, 624, 569, 642], [578, 624, 626, 643], [637, 624, 696, 642], [706, 622, 738, 642], [747, 622, 806, 647], [815, 624, 865, 647], [875, 622, 907, 642], [917, 629, 925, 642], [934, 625, 975, 642], [985, 625, 992, 642], [2, 657, 52, 681], [62, 659, 118, 680], [130, 658, 171, 676], [181, 662, 188, 676], [197, 662, 222, 677], [232, 657, 278, 680], [290, 662, 306, 676], [316, 658, 366, 676], [375, 662, 383, 676], [392, 662, 417, 676], [426, 657, 499, 680], [510, 662, 535, 676], [543, 657, 603, 676], [612, 659, 628, 676], [638, 657, 704, 681], [714, 658, 738, 676], [747, 658, 805, 676], [816, 657, 907, 676], [917, 658, 942, 676], [950, 658, 984, 676], [3, 691, 18, 710], [28, 691, 120, 715], [130, 691, 213, 710], [223, 692, 247, 710], [256, 692, 306, 710], [316, 691, 331, 710], [342, 696, 357, 710], [367, 692, 433, 710], [443, 692, 484, 715], [494, 696, 509, 710], [519, 691, 551, 710], [561, 696, 594, 710], [604, 691, 696, 711], [706, 692, 744, 714], [756, 692, 797, 710], [808, 693, 840, 715], [850, 692, 907, 710], [917, 696, 933, 710], [942, 691, 992, 710], [2, 730, 18, 750], [28, 730, 52, 744], [62, 726, 137, 750], [146, 726, 171, 744], [181, 725, 272, 744], [282, 726, 355, 750], [367, 725, 442, 750], [451, 726, 507, 748], [519, 728, 558, 750], [570, 726, 594, 744], [604, 726, 668, 748], [680, 728, 696, 744], [705, 726, 721, 744], [731, 730, 763, 744], [772, 730, 806, 744], [815, 726, 880, 750], [892, 728, 983, 744], [3, 759, 18, 778], [27, 762, 43, 778], [52, 759, 85, 778], [96, 760, 111, 778], [121, 760, 151, 784], [164, 760, 188, 778], [197, 760, 238, 778], [248, 759, 315, 778], [324, 762, 340, 778], [349, 760, 382, 778], [392, 759, 492, 784], [502, 762, 518, 778], [528, 759, 644, 778], [655, 759, 738, 778], [748, 759, 780, 778], [790, 760, 814, 778], [823, 759, 905, 778], [918, 760, 949, 778], [959, 759, 992, 784], [2, 799, 10, 812], [19, 795, 35, 818], [45, 793, 111, 818], [121, 799, 137, 812], [146, 799, 154, 812], [164, 795, 188, 812], [197, 793, 280, 818], [291, 795, 324, 812], [333, 792, 416, 816], [426, 799, 451, 812], [459, 796, 475, 812], [484, 795, 535, 812], [545, 799, 569, 812], [578, 793, 637, 818], [646, 796, 662, 812], [671, 796, 721, 812], [732, 793, 823, 812], [833, 793, 924, 818], [934, 795, 967, 812], [977, 799, 992, 812], [2, 833, 10, 846], [19, 829, 61, 852], [70, 827, 194, 851], [205, 827, 296, 851], [308, 829, 332, 846], [341, 829, 414, 852], [426, 830, 450, 846], [460, 829, 524, 852], [535, 827, 611, 846], [620, 827, 679, 846], [688, 829, 730, 846], [740, 838, 754, 841], [765, 833, 780, 846], [789, 830, 815, 846], [824, 829, 848, 846], [858, 829, 899, 846], [909, 838, 924, 841], [934, 829, 958, 846], [969, 827, 992, 852], [3, 862, 44, 881], [54, 862, 119, 881], [130, 863, 154, 881], [163, 862, 221, 881], [232, 862, 329, 881], [341, 862, 366, 881], [376, 867, 414, 885], [425, 862, 467, 881], [478, 862, 552, 881], [561, 867, 586, 881], [594, 867, 627, 886], [638, 863, 653, 881], [664, 862, 712, 881], [721, 862, 755, 881], [765, 862, 813, 886], [824, 862, 848, 881], [857, 862, 899, 881], [909, 862, 992, 881], [2, 896, 41, 920], [54, 898, 69, 915], [78, 896, 143, 919], [154, 901, 171, 915], [181, 896, 264, 915], [274, 901, 281, 915], [291, 901, 315, 915], [324, 898, 339, 915], [350, 896, 456, 919], [468, 896, 518, 915], [528, 896, 600, 920], [611, 898, 627, 915], [637, 896, 687, 915], [697, 901, 721, 915], [731, 896, 806, 920], [815, 896, 874, 915], [883, 898, 899, 915], [909, 896, 925, 915], [934, 896, 967, 920], [976, 901, 992, 915], [2, 935, 10, 949], [19, 930, 67, 954], [78, 930, 111, 949], [121, 930, 187, 954], [197, 935, 205, 949], [214, 930, 264, 954], [273, 930, 314, 949], [324, 935, 348, 949], [358, 930, 374, 949], [383, 930, 442, 949], [451, 931, 467, 949], [477, 930, 493, 949], [502, 930, 583, 953], [595, 935, 628, 949], [637, 930, 710, 953], [722, 930, 738, 949], [747, 928, 875, 954], [883, 933, 908, 949], [917, 931, 933, 949], [942, 930, 992, 949], [2, 969, 24, 989], [37, 965, 52, 983], [62, 967, 94, 983], [105, 964, 129, 983], [138, 969, 177, 987], [188, 964, 230, 983], [240, 964, 272, 989], [281, 967, 297, 983], [308, 964, 399, 983], [410, 969, 433, 983], [443, 964, 501, 989], [510, 969, 526, 983], [537, 964, 569, 983], [579, 964, 594, 983], [605, 964, 653, 983], [663, 964, 728, 989], [739, 964, 797, 989], [807, 964, 839, 983], [849, 964, 916, 989], [926, 964, 992, 983]]\n"
     ]
    }
   ],
   "source": [
    "print(words)\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv2Processor, LayoutLMv2ForQuestionAnswering\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the processor and modelprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "\n",
    "model = LayoutLMv2ForQuestionAnswering.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "\n",
    "# Load an image\n",
    "image = Image.open(\"cloud/test.png\").convert(\"RGB\")\n",
    "\n",
    "# Define the question\n",
    "question = \"What is the question you want to ask?\"\n",
    "\n",
    "# Process the image and question\n",
    "encoding = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**encoding)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Get the most likely beginning and end of answer with the argmax of the logits\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits)\n",
    "\n",
    "# Convert the tokens to the answer\n",
    "all_tokens = processor.tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze().tolist())\n",
    "answer = \" \".join(all_tokens[start_index:end_index+1])\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### naver-clova-ix/donut-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# # Load the processor and model\n",
    "# processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "import re\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# load document image from the DocVQA dataset\n",
    "dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n",
    "image = dataset[0][\"image\"]\n",
    "\n",
    "# prepare decoder inputs\n",
    "task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n",
    "question = \"When is the coffee break?\"\n",
    "prompt = task_prompt.replace(\"{user_input}\", question)\n",
    "decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "outputs = model.generate(\n",
    "    pixel_values.to(device),\n",
    "    decoder_input_ids=decoder_input_ids.to(device),\n",
    "    max_length=model.decoder.config.max_position_embeddings,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "print(processor.token2json(sequence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### microsoft/layoutlm-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nLayoutLMv2Model requires the detectron2 library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the processor and model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m processor \u001b[38;5;241m=\u001b[39m LayoutLMv2Processor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/layoutlmv2-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLayoutLMv2ForQuestionAnswering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/layoutlmv2-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4130\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4124\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   4125\u001b[0m         config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   4126\u001b[0m     )\n\u001b[1;32m   4128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   4129\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 4130\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4132\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   4133\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py:1277\u001b[0m, in \u001b[0;36mLayoutLMv2ForQuestionAnswering.__init__\u001b[0;34m(self, config, has_visual_segment_embedding)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mnum_labels\n\u001b[1;32m   1276\u001b[0m config\u001b[38;5;241m.\u001b[39mhas_visual_segment_embedding \u001b[38;5;241m=\u001b[39m has_visual_segment_embedding\n\u001b[0;32m-> 1277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayoutlmv2 \u001b[38;5;241m=\u001b[39m \u001b[43mLayoutLMv2Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mnum_labels)\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py:700\u001b[0m, in \u001b[0;36mLayoutLMv2Model.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m--> 700\u001b[0m     \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdetectron2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nLayoutLMv2Model requires the detectron2 library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv2Processor, LayoutLMv2ForQuestionAnswering\n",
    "\n",
    "# Load the processor and model\n",
    "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "model = LayoutLMv2ForQuestionAnswering.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### naver-clova-ix/donut-base-finetuned-docvqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the processor and model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mDonutProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnaver-clova-ix/donut-base-finetuned-docvqa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m VisionEncoderDecoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaver-clova-ix/donut-base-finetuned-docvqa\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load an image\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/processing_utils.py:974\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 974\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/processing_utils.py:1020\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1018\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[0;32m-> 1020\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattribute_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:921\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    919\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m         )\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2032\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2030\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2043\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2272\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2272\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2274\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2277\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:108\u001b[0m, in \u001b[0;36mXLMRobertaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     94\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m ):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     mask_token \u001b[38;5;241m=\u001b[39m AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:107\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m added_tokens_decoder \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded_tokens_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_slow \u001b[38;5;129;01mand\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot instantiate this tokenizer from a slow version. If it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms based on sentencepiece, make sure you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave sentencepiece installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed."
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load the processor and model\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "\n",
    "# Load an image\n",
    "image = Image.open(\"cloud/test.png\").convert(\"RGB\")\n",
    "\n",
    "# Define the question\n",
    "question = \"What is the question you want to ask?\"\n",
    "\n",
    "# Process the image and question\n",
    "encoding = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**encoding)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Get the most likely beginning and end of answer with the argmax of the logits\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits)\n",
    "\n",
    "# Convert the tokens to the answer\n",
    "all_tokens = processor.tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze().tolist())\n",
    "answer = \" \".join(all_tokens[start_index:end_index+1])\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai/clip-vit-base-patch32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probability of: tensor([[0.9737, 0.0263]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image = Image.open(\"cloud/catandbaby.jpg\")\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image \n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "\n",
    "print(\"Label probability of:\", probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salesforce/blip2-flan-t5-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vai/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the poster for the movie frank vs frank\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16\n",
    ")  # doctest: +IGNORE_RESULT\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# image = Image.open(\"cloud/catandbaby.jpg\").convert(\"RGB\")\n",
    "image = Image.open(\"cloud/demo_pic.jpeg\")\n",
    "\n",
    "\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(pixel_values=inputs[\"pixel_values\"])\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dandelin/vilt-b32-finetuned-vqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SequenceClassifierOutput' object has no attribute 'start_logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoding)\n\u001b[0;32m---> 19\u001b[0m start_logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_logits\u001b[49m\n\u001b[1;32m     20\u001b[0m end_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mend_logits\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Get the most likely beginning and end of answer with the argmax of the logits\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'start_logits'"
     ]
    }
   ],
   "source": [
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "from PIL import Image\n",
    "\n",
    "# Load the processor and model\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Load an image\n",
    "image = Image.open(\"cloud/test.png\").convert(\"RGB\")\n",
    "\n",
    "# Define the question\n",
    "question = \"What is the question you want to ask?\"\n",
    "\n",
    "# Process the image and question\n",
    "encoding = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**encoding)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Get the most likely beginning and end of answer with the argmax of the logits\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits)\n",
    "\n",
    "# Convert the tokens to the answer\n",
    "all_tokens = processor.tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze().tolist())\n",
    "answer = \" \".join(all_tokens[start_index:end_index+1])\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai/clip-vit-large-patch14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load the processor and model\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Example usage\n",
    "image = Image.open(\"cloud/catandbaby.jpg\").convert(\"RGB\")\n",
    "inputs = processor(text=[\"a photo of a cat\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the probabilities\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deepset/roberta-base-squad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Your question here.\n",
      "Answer: Your context here.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Example usage\n",
    "context = \"Your context here.\"\n",
    "question = \"Your question here.\"\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### timpal0l/mdeberta-v3-base-squad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Your question here.\n",
      "Answer:  context\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"timpal0l/mdeberta-v3-base-squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"timpal0l/mdeberta-v3-base-squad2\")\n",
    "\n",
    "# Create a question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example usage\n",
    "context = \"Your context here.\"\n",
    "question = \"Your question here.\"\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### google/tapas-base-finetuned-wtq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vai/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/tapas/tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/home/vai/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/tapas/tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Convert the logits to predictions\u001b[39;00m\n\u001b[1;32m     25\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdetach()  \u001b[38;5;66;03m# Detach the logits tensor\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m predicted_answer_coordinates, predicted_aggregation_indices \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_logits_to_predictions(\n\u001b[1;32m     27\u001b[0m \tinputs, logits\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Get the predicted answer\u001b[39;00m\n\u001b[1;32m     31\u001b[0m answer_coordinates \u001b[38;5;241m=\u001b[39m predicted_answer_coordinates[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "\n",
    "# Example table\n",
    "data = {\n",
    "\t\"Actors\": [\"Brad Pitt\", \"Leonardo DiCaprio\", \"George Clooney\"],\n",
    "\t\"Number of movies\": [\"87\", \"53\", \"69\"]\n",
    "}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Example question\n",
    "queries = [\"How many movies has George Clooney played in?\"]\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer(table=table, queries=queries, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# Get the model outputs\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Convert the logits to predictions\n",
    "logits = outputs.logits.detach()  # Detach the logits tensor\n",
    "predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(\n",
    "\tinputs, logits\n",
    ")\n",
    "\n",
    "# Get the predicted answer\n",
    "answer_coordinates = predicted_answer_coordinates[0]\n",
    "answer = table.iat[answer_coordinates[0][0], answer_coordinates[0][1]]\n",
    "\n",
    "print(f\"Question: {queries[0]}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### microsoft/tapex-large-finetuned-wtq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vai/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/tapas/tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/home/vai/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/tapas/tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Convert the logits to predictions\u001b[39;00m\n\u001b[1;32m     25\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdetach()  \u001b[38;5;66;03m# Detach the logits tensor\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m predicted_answer_coordinates, _ \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_logits_to_predictions(\n\u001b[1;32m     27\u001b[0m \tinputs, logits\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Check if the predicted answer coordinates are valid\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_answer_coordinates \u001b[38;5;129;01mand\u001b[39;00m predicted_answer_coordinates[\u001b[38;5;241m0\u001b[39m]:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "\n",
    "# Example table\n",
    "data = {\n",
    "\t\"Actors\": [\"Brad Pitt\", \"Leonardo DiCaprio\", \"George Clooney\"],\n",
    "\t\"Number of movies\": [\"87\", \"53\", \"69\"]\n",
    "}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Example question\n",
    "queries = [\"How many movies has George Clooney played in?\"]\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer(table=table, queries=queries, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# Get the model outputs\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Convert the logits to predictions\n",
    "logits = outputs.logits.detach()  # Detach the logits tensor\n",
    "predicted_answer_coordinates, _ = tokenizer.convert_logits_to_predictions(\n",
    "\tinputs, logits\n",
    ")\n",
    "\n",
    "# Check if the predicted answer coordinates are valid\n",
    "if predicted_answer_coordinates and predicted_answer_coordinates[0]:\n",
    "\tanswer_coordinates = predicted_answer_coordinates[0]\n",
    "\tanswer = table.iat[answer_coordinates[0][0], answer_coordinates[0][1]]\n",
    "else:\n",
    "\tanswer = \"No answer found\"\n",
    "\n",
    "print(f\"Question: {queries[0]}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helsinki-NLP/opus-mt-en-de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-de\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(**inputs)\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facebook/m2m100_418M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nM2M100Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/m2m100_418M\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mM2M100Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m M2M100ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nM2M100Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"facebook/m2m100_418M\"\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(**inputs)\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### google-t5/t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/t5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"google/t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"Translate English to German: Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helsinki-NLP/opus-mt-zh-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-zh-en\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"你好，你怎么样？\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(**inputs)\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facebook/nllb-200-distilled-600M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nM2M100Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/nllb-200-distilled-600M\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mM2M100Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m M2M100ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nM2M100Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(**inputs)\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summerization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facebook/bart-large-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Artificial intelligence (AI) has significantly transformed various industries over the past decade. From healthcare to finance, AI-powered tools have streamlined processes, improved decision-making, and enhanced customer experiences. Despite these advancements, challenges such as ethical considerations, data privacy concerns, and the need for transparency in AI decision- making remain critical issues.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"Artificial intelligence (AI) has significantly transformed various industries over the past decade, offering innovative solutions to complex problems. From healthcare to finance, AI-powered tools have streamlined processes, improved decision-making, and enhanced customer experiences. For instance, machine learning algorithms are being used in hospitals to predict patient outcomes and recommend personalized treatment plans, while financial institutions leverage AI for fraud detection and algorithmic trading. Despite these advancements, challenges such as ethical considerations, data privacy concerns, and the need for transparency in AI decision-making remain critical issues. As technology evolves, addressing these challenges will be crucial to ensure the responsible and equitable use of AI in society.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### google/pegasus-multi_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nPegasusTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/pegasus-multi_news\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPegasusTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m PegasusForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nPegasusTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece\n",
    "\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"google/pegasus-multi_news\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"Your text here.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Summary: {summary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
