{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers -U\n",
    "%pip install sentencepiece\n",
    "%pip install Pillow\n",
    "%pip install torch\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install tqdm\n",
    "%pip install torchtext\n",
    "%pip install torchsummary\n",
    "%pip install torchviz\n",
    "%pip install tensorboard\n",
    "%pip install tensorboardX\n",
    "%pip install torchmetrics\n",
    "%pip install pytorch-lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to securely input the Hugging Face API token\n",
    "api_token = getpass(\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### microsoft/layoutlmv2-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Set up pipeline with API key\n",
    "generator = pipeline('text-generation',\n",
    "                     api_key='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')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Concepts in the Code\n",
    "###### OCR with Pytesseract:\n",
    "Extracts words and their positions from the image.\n",
    "\n",
    "###### Bounding Box Normalization:\n",
    "Ensures coordinates match LayoutLMv2’s expected input range.\n",
    "\n",
    "###### Tokenization and Encoding:\n",
    "Prepares the image and text for input into the transformer model.\n",
    "\n",
    "###### Answer Extraction:\n",
    "Identifies the most probable span of text corresponding to the answer.\n",
    "\n",
    "###### Pretrained Model:\n",
    "Leverages LayoutLMv2’s capabilities to process structured documents and handle layout-aware questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv2ForQuestionAnswering, LayoutLMv2Processor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = LayoutLMv2ForQuestionAnswering.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "\n",
    "# Load the document image\n",
    "image_path = \"/home/vai/Desktop/chat_bot_experiment_one/cloud/test.png\"  # Replace with your document image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# Define the question and context (text from the document)\n",
    "question = \"waht is langChan format ?\"\n",
    "\n",
    "\n",
    "# Define some example text and bounding boxes\n",
    "text = [\"What\", \"is\", \"LangChain\", \"format\", \"?\"]\n",
    "bbox = [[50, 50, 150, 100], [160, 50, 240, 100], [250, 50, 350, 100], [360, 50, 460, 100], [470, 50, 520, 100]]\n",
    "\n",
    "# Process the inputs (image, text, and bounding boxes)\n",
    "encoded_inputs = processor(image, text,return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "outputs = model(**encoded_inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Extract the start and end logits for the answer\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Get the answer tokens\n",
    "start_index = torch.argmax(start_logits, dim=1).item()\n",
    "end_index = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "# Decode the answer\n",
    "tokens = encoded_inputs[\"input_ids\"].squeeze()\n",
    "answer = processor.tokenizer.decode(tokens[start_index:end_index + 1])\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fine-tunned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv2Processor, LayoutLMv2ForQuestionAnswering\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytesseract\n",
    "\n",
    "# Load model and processor\n",
    "model = LayoutLMv2ForQuestionAnswering.from_pretrained(\"tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\")\n",
    "processor = LayoutLMv2Processor.from_pretrained(\"tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\")\n",
    "\n",
    "# Load image\n",
    "image_path = \"/home/vai/Desktop/chat_bot_experiment_one/cloud/test.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Extract words and bounding boxes using OCR\n",
    "ocr_results = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "words = []\n",
    "boxes = []\n",
    "\n",
    "# Get image dimensions\n",
    "image_width, image_height = image.size\n",
    "\n",
    "for i in range(len(ocr_results['text'])):\n",
    "    if ocr_results['text'][i].strip():  # Ignore empty text\n",
    "        words.append(ocr_results['text'][i])\n",
    "        \n",
    "        # Extract bounding box coordinates\n",
    "        x, y, w, h = (ocr_results['left'][i], ocr_results['top'][i],\n",
    "                      ocr_results['width'][i], ocr_results['height'][i])\n",
    "\n",
    "        # Normalize coordinates to 0–1000 range\n",
    "        normalized_bbox = [\n",
    "            int(1000 * (x / image_width)),\n",
    "            int(1000 * (y / image_height)),\n",
    "            int(1000 * ((x + w) / image_width)),\n",
    "            int(1000 * ((y + h) / image_height))\n",
    "        ]\n",
    "        boxes.append(normalized_bbox)\n",
    "\n",
    "\n",
    "# Question\n",
    "question = \"Tell me someting about covid?\"\n",
    "\n",
    "# Encode inputs with normalized bounding boxes\n",
    "encoded_inputs = processor(\n",
    "    image,\n",
    "    words,\n",
    "    boxes=boxes,  # Normalized bounding boxes\n",
    "    return_tensors=\"pt\",\n",
    "    # truncation=True,\n",
    "    \n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "outputs = model(**encoded_inputs)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Extract the answer\n",
    "start_index = torch.argmax(start_logits, dim=1).item()\n",
    "end_index = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "tokens = encoded_inputs[\"input_ids\"].squeeze()\n",
    "answer = processor.tokenizer.decode(tokens[start_index:end_index + 1])\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### check quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[Document', '(page_content=\"This', 'transcript', 'is', 'provided', 'for', 'the', 'convenience', 'of', 'investors', 'only,', 'for', 'a', 'full', 'recording', 'pleas', 'e', 'see', 'the', 'Q4', '2021', 'Earnings', 'Call', 'webcast', '.\\\\n\\\\nAlphabet', 'Q4', '2021', 'Earnings', 'Call', 'February', '1,', '2022\\\\n\\\\nOperator:', 'Welcome', 'eve', 'ryone.', 'And', 'thank', 'you', 'for', 'standing', 'by', 'for', 'the', 'Alphabet', 'fourth', 'quarter', '2021', 'earnings', 'conference', 'call.', 'At', 'this', 'time,', 'all', 'participants', 'are', 'in', 'a', 'listen-only', 'mode.', 'After', 'the', 'speaker', 'presentation,', 'there', 'will', 'be', 'a', 'question', 'and', 'answer', 'session.', 'To', 'ask', 'a', 'question', 'during', 'the', 'session,', 'you', 'will', 'need', 'to', 'press', 'star', 'one', 'on', 'your', 'telephone.', 'If', 'you', 'require', 'any', 'further', 'a', 'ssistance,', 'please', 'press', 'star', 'zero.', 'I', 'would', 'now', 'like', 'to', 'hand', 'the', 'conference', 'over', 'to', 'your', 'speaker', 'today,', 'Jim', 'Friedland,', 'Director', 'of', 'Investor', 'Relations.', 'Please', 'go', 'ahead.\\\\n\\\\nJim', 'Friedland,', 'Director', 'Investor', 'Relations:', 'Thank', 'you.', 'Good', 'after', 'noon,', 'everyone,', 'and', 'welcome', 'to', 'Alphabet’s', 'fourth', 'quarter', '2021', 'earnings', 'conference', 'call.', 'With', 'us', 'today', 'are', 'Sundar', 'Pich', 'ai,', 'Philipp', 'Schindler', 'and', 'Ruth', 'Porat.', 'Now', 'I‘1ll', 'quickly', 'cover', 'the', 'Safe', 'Harbor.', 'Some', 'of', 'the', 'statements', 'that', 'we', 'make', 'tod', 'ay', 'regarding', 'our', 'business,', 'operations,', 'and', 'financial', 'performance,', 'including', 'the', 'effect', 'of', 'the', 'COVID-19', 'pandemic', 'on', 'th', 'ose', 'areas,', 'may', 'be', 'considered', 'forward-looking,', 'and', 'such', 'statements', 'involve', 'a', 'number', 'of', 'risks', 'and', 'uncertainties', 'that', 'co', 'uld', 'cause', 'actual', 'results', 'to', 'differ', 'materially.', 'For', 'more', 'information,', 'please', 'refer', 'to', 'the', 'risk', 'factors', 'discussed', 'in', 'ou', 'x', 'Forms', '10-K', 'and', '10-Q', 'filed', 'with', 'the', 'SEC,', 'including', 'our', 'upcoming', 'Form', '10-K', 'filing', 'for', 'the', 'year', 'ended', 'December', '31,', '202', '1.', 'During', 'this', 'call,', 'we', 'will', 'present', 'both', 'GAAP', 'and', 'non-GAAP', 'financial', 'measures.', 'A', 'reconciliation', 'of', 'non-GAAP', 'to', 'GAAP', 'measures', 'is', 'included', 'in', \"today's\", 'press', 'release,', 'which', 'is', 'distributed', 'and', 'available', 'to', 'the', 'public', 'through', 'our', 'Investor', 'Relations', 'website', 'located', 'at', 'abc.xyz/investor.', 'And', 'now', 'I‘1l', 'turn', 'the', 'call', 'over', 'to', 'Sundar.\\\\n\\\\nSundar', 'Pichai,', 'CEO', 'Alpha', 'bet', 'and', 'Google:', 'Thank', 'you,', 'Jim,', 'and', 'Happy', 'New', 'Year,', 'everyone.', 'The', 'last', 'few', 'months', 'have', 'been', 'challenging', 'for', 'communiti', 'es', 'everywhere', 'because', 'of', 'Omicron.', 'I’m', 'grateful', 'for', 'the', 'frontline', 'healthcare', 'workers', 'who', 'are', 'helping', 'us', 'through', 'it,', 'an', 'd', 'glad', 'to', 'see', 'signs', 'that', 'this', 'wave', 'is', 'receding', 'in', 'many', 'parts', 'of', 'the', 'world.', 'Whether', 'it’s', 'helping', 'people', 'find', 'a', 'COVID', 't', 'esting', 'center,', 'learn', 'a', 'new', 'skill,', 'or', 'launch', 'a', 'new', 'business,', 'our', 'mission', 'to', 'organize', 'the', 'world’s', 'information', 'and', 'make', 'it', 'universally', 'accessible', 'and', 'useful', 'is', 'as', 'relevant', 'today', 'as', 'it’s', 'ever', 'been.\\\\n\\\\nIn', '2022,', \"we'll\", 'stay', 'focused', 'on', 'evolvi', 'ng', 'our', 'knowledge', 'and', 'information', 'products,', 'including', 'Search,', 'Maps,', 'and', 'YouTube,', 'to', 'be', 'even', 'more', 'helpful.', 'Investments', 'in', 'AI', 'will', 'be', 'key,', 'and', 'we’ll', 'continue', 'to', 'make', 'improvements', 'to', 'conversational', 'interfaces', 'like', 'the', 'Assistant.', 'I’1l', 'begi', 'n', 'by', 'touching', 'on', 'a', 'few', 'highlights', 'from', 'Q4.\\\\n\\\\nOur', 'new', 'AI', 'models', 'are', 'helping', 'to', 'create', 'information', 'experiences', 'that', 'ar', 'e', 'truly', 'conversational,', 'multimodal,', 'and', 'personal.', 'For', 'example,', 'Multitask', 'Unified', 'Model', '--', 'or', 'MUM', 'for', 'short', '--', 'has', 'imp', 'roved', 'searches', 'for', 'vaccine', 'information.', 'And', 'soon,', \"we'll\", 'introduce', 'new', 'ways', 'to', 'search', 'with', 'images', 'and', 'words', 'simultaneo', 'usly.', 'In', 'October,', 'we', 'introduced', 'a', 'new', 'AI', 'architecture,', 'called', 'Pathways.', 'AI', 'models', 'are', 'typically', 'trained', 'to', 'do', 'only', 'on', 'e', 'thing.', 'With', 'Pathways', 'a', 'single', 'model', 'can', 'be', 'trained', 'to', 'do', 'thousands,', 'even', 'millions,', 'of', 'things.\\\\n\\\\nFrom', 'MUM', 'to', 'Pathwa', 'ys,', 'to', 'BERT', 'and', 'more,', 'these', 'deep', 'AI', 'investments', 'are', 'helping', 'us', 'lead', 'in', 'search', 'quality.', 'They’re', 'also', 'powering', 'innovati']\n",
      "[[4, 12, 77, 34], [81, 9, 238, 35], [248, 9, 331, 34], [342, 9, 357, 28], [366, 9, 434, 34], [443, 10, 467, 28], [477, 10, 501, 28], [510, 9, 603, 28], [612, 10, 627, 28], [638, 9, 712, 28], [722, 10, 761, 34], [774, 10, 798, 28], [807, 15, 815, 28], [824, 10, 857, 28], [866, 9, 942, 34], [951, 10, 991, 34], [2, 49, 10, 62], [20, 49, 43, 62], [53, 45, 77, 62], [87, 45, 102, 66], [113, 45, 144, 62], [155, 43, 221, 68], [231, 45, 263, 62], [273, 45, 331, 62], [344, 42, 450, 68], [460, 45, 475, 66], [486, 45, 517, 62], [528, 43, 594, 68], [604, 45, 636, 62], [646, 45, 713, 68], [723, 45, 736, 66], [748, 42, 888, 68], [899, 45, 958, 62], [968, 49, 992, 62], [3, 83, 50, 102], [61, 79, 86, 96], [96, 79, 137, 96], [146, 83, 171, 102], [181, 79, 205, 96], [214, 77, 281, 102], [290, 79, 307, 102], [316, 79, 340, 96], [350, 79, 374, 96], [383, 79, 450, 102], [460, 79, 510, 96], [519, 80, 577, 102], [587, 79, 619, 96], [629, 77, 695, 102], [706, 79, 789, 96], [799, 79, 837, 96], [848, 80, 865, 96], [875, 77, 907, 96], [917, 77, 956, 101], [968, 79, 992, 96], [2, 112, 103, 136], [113, 117, 137, 131], [147, 112, 162, 131], [172, 117, 179, 131], [189, 112, 281, 136], [290, 113, 329, 131], [341, 113, 382, 131], [392, 113, 416, 131], [427, 113, 484, 136], [493, 112, 600, 136], [612, 113, 653, 131], [662, 112, 695, 131], [705, 113, 721, 131], [731, 117, 739, 131], [747, 112, 815, 136], [824, 113, 848, 131], [858, 117, 907, 131], [918, 112, 982, 131], [2, 148, 18, 165], [28, 147, 52, 165], [62, 151, 69, 165], [78, 146, 145, 170], [155, 146, 205, 170], [214, 147, 239, 165], [249, 146, 312, 169], [324, 151, 348, 170], [357, 146, 390, 165], [401, 146, 434, 165], [443, 148, 459, 165], [468, 151, 509, 170], [519, 148, 552, 165], [561, 151, 585, 165], [595, 151, 611, 165], [620, 151, 653, 170], [663, 146, 744, 170], [757, 147, 771, 165], [781, 151, 806, 170], [815, 146, 874, 170], [883, 151, 907, 170], [918, 146, 975, 165], [985, 151, 992, 165], [3, 180, 84, 203], [96, 180, 145, 204], [155, 185, 195, 204], [206, 183, 239, 199], [249, 185, 287, 199], [300, 183, 306, 199], [315, 180, 357, 199], [367, 185, 392, 199], [401, 180, 433, 199], [442, 181, 459, 199], [468, 180, 501, 199], [510, 180, 535, 199], [544, 180, 628, 199], [638, 185, 671, 199], [680, 183, 696, 199], [705, 185, 738, 204], [748, 180, 806, 204], [815, 180, 863, 204], [875, 180, 900, 199], [909, 180, 990, 203], [2, 214, 69, 233], [78, 214, 94, 233], [105, 215, 171, 233], [180, 214, 262, 233], [274, 214, 323, 233], [333, 219, 348, 239], [359, 213, 468, 234], [477, 214, 558, 237], [570, 214, 636, 233], [646, 215, 712, 233], [722, 214, 804, 233], [815, 214, 857, 233], [866, 219, 896, 239], [908, 214, 942, 233], [951, 214, 992, 233], [2, 254, 41, 271], [53, 254, 126, 273], [138, 248, 162, 267], [171, 248, 230, 267], [240, 250, 256, 267], [265, 248, 348, 273], [359, 248, 408, 267], [417, 250, 475, 273], [486, 250, 517, 267], [528, 248, 594, 273], [604, 248, 687, 267], [697, 248, 736, 267], [747, 248, 780, 267], [790, 254, 805, 267], [815, 248, 857, 273], [866, 254, 890, 267], [901, 248, 950, 267], [960, 248, 992, 267], [3, 282, 24, 306], [37, 282, 94, 307], [105, 282, 179, 301], [189, 282, 213, 301], [222, 282, 256, 301], [265, 284, 312, 301], [324, 285, 349, 301], [359, 282, 390, 301], [401, 282, 459, 307], [468, 288, 509, 301], [519, 282, 543, 301], [553, 282, 585, 301], [595, 282, 651, 301], [664, 284, 696, 301], [706, 282, 721, 301], [731, 282, 755, 301], [765, 284, 848, 301], [858, 282, 890, 301], [899, 288, 916, 301], [925, 282, 958, 301], [968, 282, 993, 301], [3, 322, 18, 341], [28, 316, 103, 341], [112, 322, 137, 336], [146, 316, 219, 340], [231, 316, 321, 341], [333, 316, 357, 336], [368, 316, 441, 336], [451, 316, 550, 341], [562, 316, 636, 341], [646, 316, 671, 336], [680, 316, 730, 336], [739, 316, 754, 336], [765, 316, 789, 336], [798, 316, 864, 336], [875, 316, 941, 341], [951, 322, 967, 336], [976, 316, 992, 336], [2, 356, 26, 370], [37, 356, 84, 374], [95, 356, 120, 375], [129, 351, 145, 370], [155, 351, 239, 370], [249, 351, 380, 375], [392, 351, 416, 370], [427, 351, 459, 370], [469, 352, 551, 370], [562, 351, 620, 370], [629, 356, 637, 370], [646, 351, 696, 370], [706, 351, 721, 370], [731, 351, 771, 370], [782, 351, 807, 370], [815, 351, 924, 370], [934, 351, 967, 370], [976, 356, 992, 370], [2, 385, 27, 404], [37, 390, 77, 404], [87, 385, 136, 404], [146, 385, 204, 404], [214, 386, 230, 404], [240, 383, 289, 404], [298, 385, 389, 409], [401, 386, 425, 404], [434, 390, 467, 404], [478, 383, 575, 408], [587, 385, 636, 409], [646, 385, 687, 404], [697, 386, 712, 404], [722, 385, 747, 404], [756, 383, 789, 404], [799, 385, 856, 404], [866, 385, 942, 404], [951, 383, 967, 404], [976, 390, 992, 404], [2, 424, 10, 438], [19, 420, 60, 438], [71, 419, 103, 438], [113, 419, 137, 438], [148, 419, 179, 441], [189, 418, 230, 438], [239, 418, 272, 438], [282, 419, 306, 438], [316, 420, 346, 442], [359, 418, 434, 442], [443, 424, 467, 438], [477, 419, 543, 443], [553, 420, 587, 438], [596, 419, 628, 438], [638, 418, 688, 442], [697, 419, 721, 438], [731, 419, 755, 438], [765, 424, 798, 443], [807, 419, 848, 438], [858, 419, 925, 438], [935, 419, 956, 442], [969, 419, 991, 438], [4, 453, 16, 471], [28, 452, 78, 476], [87, 452, 119, 472], [129, 453, 168, 476], [179, 459, 196, 472], [205, 452, 238, 472], [248, 454, 306, 476], [315, 453, 348, 472], [358, 454, 391, 472], [401, 453, 425, 472], [434, 454, 501, 472], [511, 452, 585, 472], [594, 459, 668, 472], [679, 454, 688, 472], [697, 452, 815, 472], [824, 453, 839, 472], [850, 454, 916, 472], [925, 454, 941, 472], [951, 454, 984, 472], [2, 493, 68, 506], [79, 486, 94, 506], [105, 486, 171, 506], [181, 486, 196, 505], [206, 486, 263, 510], [274, 493, 314, 510], [324, 487, 389, 509], [400, 486, 442, 506], [452, 486, 467, 506], [477, 486, 569, 506], [578, 487, 603, 506], [613, 486, 687, 506], [697, 489, 712, 506], [722, 487, 747, 506], [756, 486, 806, 510], [815, 487, 874, 510], [883, 493, 907, 506], [918, 489, 984, 506], [2, 520, 77, 540], [87, 520, 145, 540], [155, 521, 213, 540], [223, 523, 239, 540], [248, 520, 389, 545], [400, 521, 425, 539], [434, 527, 459, 540], [469, 521, 501, 539], [510, 523, 543, 539], [553, 521, 577, 540], [587, 521, 619, 540], [629, 527, 662, 540], [671, 523, 688, 540], [697, 520, 839, 540], [849, 520, 905, 545], [917, 523, 942, 540], [950, 521, 993, 545], [2, 556, 26, 573], [37, 556, 61, 573], [70, 556, 126, 579], [138, 556, 179, 573], [188, 561, 219, 579], [231, 554, 262, 577], [274, 556, 298, 573], [307, 557, 348, 579], [358, 557, 383, 575], [392, 557, 431, 579], [443, 561, 516, 579], [528, 556, 552, 575], [562, 556, 594, 573], [605, 556, 629, 575], [637, 556, 687, 573], [697, 556, 730, 575], [739, 556, 772, 575], [782, 554, 874, 579], [884, 556, 907, 573], [917, 554, 992, 575], [2, 595, 17, 607], [28, 590, 111, 613], [121, 590, 179, 607], [188, 590, 204, 607], [214, 588, 278, 607], [291, 591, 315, 607], [324, 590, 390, 613], [401, 590, 425, 607], [434, 590, 458, 607], [469, 588, 543, 607], [553, 590, 636, 607], [645, 590, 703, 609], [713, 590, 738, 609], [748, 595, 772, 607], [782, 588, 840, 613], [849, 595, 864, 607], [875, 590, 933, 613], [943, 588, 964, 612], [977, 595, 992, 607], [2, 624, 10, 642], [19, 624, 52, 647], [62, 625, 78, 642], [87, 628, 111, 642], [122, 622, 162, 647], [172, 624, 204, 642], [214, 622, 246, 642], [256, 628, 289, 643], [300, 622, 314, 642], [324, 622, 391, 647], [401, 622, 416, 642], [425, 629, 459, 647], [468, 625, 508, 647], [519, 624, 535, 642], [544, 624, 569, 642], [578, 624, 626, 643], [637, 624, 696, 642], [706, 622, 738, 642], [747, 622, 806, 647], [815, 624, 865, 647], [875, 622, 907, 642], [917, 629, 925, 642], [934, 625, 975, 642], [985, 625, 992, 642], [2, 657, 52, 681], [62, 659, 118, 680], [130, 658, 171, 676], [181, 662, 188, 676], [197, 662, 222, 677], [232, 657, 278, 680], [290, 662, 306, 676], [316, 658, 366, 676], [375, 662, 383, 676], [392, 662, 417, 676], [426, 657, 499, 680], [510, 662, 535, 676], [543, 657, 603, 676], [612, 659, 628, 676], [638, 657, 704, 681], [714, 658, 738, 676], [747, 658, 805, 676], [816, 657, 907, 676], [917, 658, 942, 676], [950, 658, 984, 676], [3, 691, 18, 710], [28, 691, 120, 715], [130, 691, 213, 710], [223, 692, 247, 710], [256, 692, 306, 710], [316, 691, 331, 710], [342, 696, 357, 710], [367, 692, 433, 710], [443, 692, 484, 715], [494, 696, 509, 710], [519, 691, 551, 710], [561, 696, 594, 710], [604, 691, 696, 711], [706, 692, 744, 714], [756, 692, 797, 710], [808, 693, 840, 715], [850, 692, 907, 710], [917, 696, 933, 710], [942, 691, 992, 710], [2, 730, 18, 750], [28, 730, 52, 744], [62, 726, 137, 750], [146, 726, 171, 744], [181, 725, 272, 744], [282, 726, 355, 750], [367, 725, 442, 750], [451, 726, 507, 748], [519, 728, 558, 750], [570, 726, 594, 744], [604, 726, 668, 748], [680, 728, 696, 744], [705, 726, 721, 744], [731, 730, 763, 744], [772, 730, 806, 744], [815, 726, 880, 750], [892, 728, 983, 744], [3, 759, 18, 778], [27, 762, 43, 778], [52, 759, 85, 778], [96, 760, 111, 778], [121, 760, 151, 784], [164, 760, 188, 778], [197, 760, 238, 778], [248, 759, 315, 778], [324, 762, 340, 778], [349, 760, 382, 778], [392, 759, 492, 784], [502, 762, 518, 778], [528, 759, 644, 778], [655, 759, 738, 778], [748, 759, 780, 778], [790, 760, 814, 778], [823, 759, 905, 778], [918, 760, 949, 778], [959, 759, 992, 784], [2, 799, 10, 812], [19, 795, 35, 818], [45, 793, 111, 818], [121, 799, 137, 812], [146, 799, 154, 812], [164, 795, 188, 812], [197, 793, 280, 818], [291, 795, 324, 812], [333, 792, 416, 816], [426, 799, 451, 812], [459, 796, 475, 812], [484, 795, 535, 812], [545, 799, 569, 812], [578, 793, 637, 818], [646, 796, 662, 812], [671, 796, 721, 812], [732, 793, 823, 812], [833, 793, 924, 818], [934, 795, 967, 812], [977, 799, 992, 812], [2, 833, 10, 846], [19, 829, 61, 852], [70, 827, 194, 851], [205, 827, 296, 851], [308, 829, 332, 846], [341, 829, 414, 852], [426, 830, 450, 846], [460, 829, 524, 852], [535, 827, 611, 846], [620, 827, 679, 846], [688, 829, 730, 846], [740, 838, 754, 841], [765, 833, 780, 846], [789, 830, 815, 846], [824, 829, 848, 846], [858, 829, 899, 846], [909, 838, 924, 841], [934, 829, 958, 846], [969, 827, 992, 852], [3, 862, 44, 881], [54, 862, 119, 881], [130, 863, 154, 881], [163, 862, 221, 881], [232, 862, 329, 881], [341, 862, 366, 881], [376, 867, 414, 885], [425, 862, 467, 881], [478, 862, 552, 881], [561, 867, 586, 881], [594, 867, 627, 886], [638, 863, 653, 881], [664, 862, 712, 881], [721, 862, 755, 881], [765, 862, 813, 886], [824, 862, 848, 881], [857, 862, 899, 881], [909, 862, 992, 881], [2, 896, 41, 920], [54, 898, 69, 915], [78, 896, 143, 919], [154, 901, 171, 915], [181, 896, 264, 915], [274, 901, 281, 915], [291, 901, 315, 915], [324, 898, 339, 915], [350, 896, 456, 919], [468, 896, 518, 915], [528, 896, 600, 920], [611, 898, 627, 915], [637, 896, 687, 915], [697, 901, 721, 915], [731, 896, 806, 920], [815, 896, 874, 915], [883, 898, 899, 915], [909, 896, 925, 915], [934, 896, 967, 920], [976, 901, 992, 915], [2, 935, 10, 949], [19, 930, 67, 954], [78, 930, 111, 949], [121, 930, 187, 954], [197, 935, 205, 949], [214, 930, 264, 954], [273, 930, 314, 949], [324, 935, 348, 949], [358, 930, 374, 949], [383, 930, 442, 949], [451, 931, 467, 949], [477, 930, 493, 949], [502, 930, 583, 953], [595, 935, 628, 949], [637, 930, 710, 953], [722, 930, 738, 949], [747, 928, 875, 954], [883, 933, 908, 949], [917, 931, 933, 949], [942, 930, 992, 949], [2, 969, 24, 989], [37, 965, 52, 983], [62, 967, 94, 983], [105, 964, 129, 983], [138, 969, 177, 987], [188, 964, 230, 983], [240, 964, 272, 989], [281, 967, 297, 983], [308, 964, 399, 983], [410, 969, 433, 983], [443, 964, 501, 989], [510, 969, 526, 983], [537, 964, 569, 983], [579, 964, 594, 983], [605, 964, 653, 983], [663, 964, 728, 989], [739, 964, 797, 989], [807, 964, 839, 983], [849, 964, 916, 989], [926, 964, 992, 983]]\n"
     ]
    }
   ],
   "source": [
    "print(words)\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### microsoft/layoutlmv2-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of LayoutLMv2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['layoutlmv2.visual_segment_embedding', 'qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'boxes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the question you want to ask?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Process the image and question\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoding)\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/layoutlmv2/processing_layoutlmv2.py:128\u001b[0m, in \u001b[0;36mLayoutLMv2Processor.__call__\u001b[0;34m(self, images, text, text_pair, boxes, word_labels, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m         text \u001b[38;5;241m=\u001b[39m [text]  \u001b[38;5;66;03m# add batch dimension (as the image processor always adds a batch dimension)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     text_pair \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    125\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    126\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    127\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m--> 128\u001b[0m     boxes\u001b[38;5;241m=\u001b[39mboxes \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    129\u001b[0m     word_labels\u001b[38;5;241m=\u001b[39mword_labels,\n\u001b[1;32m    130\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    131\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m    132\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m    133\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    134\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m    135\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    136\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    137\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    138\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    139\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    140\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    141\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    142\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    143\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    145\u001b[0m )\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# add pixel values\u001b[39;00m\n\u001b[1;32m    148\u001b[0m images \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:87\u001b[0m, in \u001b[0;36mBatchFeature.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_values', 'attention_mask',\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing with integers is not available when using Python based feature extractors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'boxes'"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv2Processor, LayoutLMv2ForQuestionAnswering\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the processor and modelprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "\n",
    "model = LayoutLMv2ForQuestionAnswering.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "\n",
    "# Load an image\n",
    "image = Image.open(\"cloud/test.png\").convert(\"RGB\")\n",
    "\n",
    "# Define the question\n",
    "question = \"What is the question you want to ask?\"\n",
    "\n",
    "# Process the image and question\n",
    "encoding = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**encoding)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Get the most likely beginning and end of answer with the argmax of the logits\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits)\n",
    "\n",
    "# Convert the tokens to the answer\n",
    "all_tokens = processor.tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze().tolist())\n",
    "answer = \" \".join(all_tokens[start_index:end_index+1])\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naver-clova-ix/donut-base\n",
    "\n",
    "##### Step-by-step Document Image Classification\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n",
    "\n",
    "##### Step-by-step Document Parsing\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\").\n",
    "\n",
    "##### Step-by-step Document Visual Question Answering (DocVQA)\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### naver-clova-ix/donut-base-finetuned-docvqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    14,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": [\n",
      "    2560,\n",
      "    1920\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"donut-swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 10\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57532\n",
      "}\n",
      "\n",
      "Generating test split: 100%|██████████| 3/3 [00:00<00:00, 65.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'When is the coffee break?', 'answer': '11-14 to 11:39 a.m.'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "\n",
    "# # Load the processor and model\n",
    "# processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# load document image from the DocVQA dataset\n",
    "# dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n",
    "dataset = load_dataset(\"opendatalab/OmniDocBench\", split=\"test\")\n",
    "\n",
    "\n",
    "image = dataset[0][\"image\"]\n",
    "\n",
    "# prepare decoder inputs\n",
    "task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n",
    "question = \"When is the coffee break?\"\n",
    "prompt = task_prompt.replace(\"{user_input}\", question)\n",
    "decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "outputs = model.generate(\n",
    "    pixel_values.to(device),\n",
    "    decoder_input_ids=decoder_input_ids.to(device),\n",
    "    max_length=model.decoder.config.max_position_embeddings,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "print(processor.token2json(sequence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai/clip-vit-base-patch32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probability of: tensor([[0.9737, 0.0263]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image = Image.open(\"cloud/catandbaby.jpg\")\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image \n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "\n",
    "print(\"Label probability of:\", probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salesforce/blip2-flan-t5-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the poster for the movie frank vs frank\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16\n",
    ")  # doctest: +IGNORE_RESULT\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# image = Image.open(\"cloud/catandbaby.jpg\").convert(\"RGB\")\n",
    "image = Image.open(\"cloud/demo_pic.jpeg\")\n",
    "\n",
    "\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(pixel_values=inputs[\"pixel_values\"])\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dandelin/vilt-b32-finetuned-vqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "from PIL import Image\n",
    "\n",
    "# Load the processor and model\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Load an image\n",
    "image = Image.open(\"cloud/test.png\").convert(\"RGB\")\n",
    "\n",
    "# Define the question\n",
    "question = \"What is the question you want to ask?\"\n",
    "\n",
    "# Process the image and question\n",
    "encoding = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**encoding)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Get the most likely beginning and end of answer with the argmax of the logits\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits)\n",
    "\n",
    "# Convert the tokens to the answer\n",
    "all_tokens = processor.tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze().tolist())\n",
    "answer = \" \".join(all_tokens[start_index:end_index+1])\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai/clip-vit-large-patch14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load the processor and model\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Example usage\n",
    "image = Image.open(\"cloud/catandbaby.jpg\").convert(\"RGB\")\n",
    "inputs = processor(text=[\"a photo of a cat\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the probabilities\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deepset/roberta-base-squad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which party win the election?\n",
      "Answer: Awami League\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Example usage\n",
    "context = \"\"\"The 2025 Bangladesh General Election is set to take place on January 15, 2025, as scheduled by the Election Commission of Bangladesh. This election will determine the composition of the Jatiya Sangsad (National Parliament) for the next five years.Key Parties and Candidates Awami League (AL) - The ruling party, led by Sheikh Hasina, is campaigning on its track record of economic growth, infrastructure development, and social reforms.\n",
    "Bangladesh Nationalist Party (BNP) - The main opposition, led by Tarique Rahman, is focusing on anti-corruption measures, electoral reforms, and strengthening democracy.\n",
    "Jatiya Party (JP) - A centrist party, led by GM Quader, aims to act as a kingmaker if no party wins a majority.\n",
    "New Progressive Alliance (NPA) - A coalition of smaller parties and independent candidates, advocating for youth employment, climate change mitigation, and digital governance.\n",
    "Major Issues in the Election\n",
    "Economic Challenges: High inflation, unemployment, and trade deficits have become key concerns for voters.\n",
    "Democratic Reforms: Opposition parties are demanding reforms in election procedures, including the deployment of international observers.\n",
    "Infrastructure and Development: The government highlights major projects like the Padma Bridge and Metro Rail as signs of progress.\n",
    "Climate Change and Resilience: Bangladesh's vulnerability to natural disasters and rising sea levels remains a major issue.\n",
    "Youth Empowerment: Parties are addressing demands for modern education reforms, digital jobs, and entrepreneurship opportunities.\n",
    "Security and Monitoring\n",
    "The Election Commission has announced the deployment of 70,000 security personnel, including the Rapid Action Battalion (RAB), to ensure a peaceful election. Over 100 international observers and local NGOs will monitor voting transparency.\n",
    "\n",
    "Opinion Polls\n",
    "Awami League: 45%\n",
    "BNP: 35%\n",
    "Jatiya Party: 10%\n",
    "Undecided Voters: 10%\n",
    "Controversies\n",
    "Election Integrity - The opposition has accused the government of bias in election management and called for a caretaker government to oversee the polls.\n",
    "Media Freedom - Allegations of media censorship and intimidation of journalists have sparked debates on press freedom.\n",
    "Violence - Several clashes between supporters of different parties have been reported during pre-election rallies.\n",
    "Expected Outcomes\n",
    "While Awami League is projected to retain power based on current trends, BNP is expected to increase its parliamentary seats, potentially leading to a more competitive and divided parliament.\"\"\"\n",
    "question = \"Which party win the election?\"\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### timpal0l/mdeberta-v3-base-squad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Jatiya Party percentage?\n",
      "Answer:  10%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"timpal0l/mdeberta-v3-base-squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"timpal0l/mdeberta-v3-base-squad2\")\n",
    "\n",
    "# Create a question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example usage\n",
    "context = \"\"\"The 2025 Bangladesh General Election is set to take place on January 15, 2025, as scheduled by the Election Commission of Bangladesh. This election will determine the composition of the Jatiya Sangsad (National Parliament) for the next five years.Key Parties and Candidates Awami League (AL) - The ruling party, led by Sheikh Hasina, is campaigning on its track record of economic growth, infrastructure development, and social reforms.\n",
    "Bangladesh Nationalist Party (BNP) - The main opposition, led by Tarique Rahman, is focusing on anti-corruption measures, electoral reforms, and strengthening democracy.\n",
    "Jatiya Party (JP) - A centrist party, led by GM Quader, aims to act as a kingmaker if no party wins a majority.\n",
    "New Progressive Alliance (NPA) - A coalition of smaller parties and independent candidates, advocating for youth employment, climate change mitigation, and digital governance.\n",
    "Major Issues in the Election\n",
    "Economic Challenges: High inflation, unemployment, and trade deficits have become key concerns for voters.\n",
    "Democratic Reforms: Opposition parties are demanding reforms in election procedures, including the deployment of international observers.\n",
    "Infrastructure and Development: The government highlights major projects like the Padma Bridge and Metro Rail as signs of progress.\n",
    "Climate Change and Resilience: Bangladesh's vulnerability to natural disasters and rising sea levels remains a major issue.\n",
    "Youth Empowerment: Parties are addressing demands for modern education reforms, digital jobs, and entrepreneurship opportunities.\n",
    "Security and Monitoring\n",
    "The Election Commission has announced the deployment of 70,000 security personnel, including the Rapid Action Battalion (RAB), to ensure a peaceful election. Over 100 international observers and local NGOs will monitor voting transparency.\n",
    "\n",
    "Opinion Polls\n",
    "Awami League: 45%\n",
    "BNP: 35%\n",
    "Jatiya Party: 10%\n",
    "Undecided Voters: 10%\n",
    "Controversies\n",
    "Election Integrity - The opposition has accused the government of bias in election management and called for a caretaker government to oversee the polls.\n",
    "Media Freedom - Allegations of media censorship and intimidation of journalists have sparked debates on press freedom.\n",
    "Violence - Several clashes between supporters of different parties have been reported during pre-election rallies.\n",
    "Expected Outcomes\n",
    "While Awami League is projected to retain power based on current trends, BNP is expected to increase its parliamentary seats, potentially leading to a more competitive and divided parliament.\"\"\"\n",
    "question = \"What is Jatiya Party percentage?\"\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### google/tapas-base-finetuned-wtq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_answer_coordinates \u001b[38;5;129;01mand\u001b[39;00m predicted_answer_coordinates[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m     32\u001b[0m \tanswer_coordinates \u001b[38;5;241m=\u001b[39m predicted_answer_coordinates[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 33\u001b[0m \tanswer \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39miat[answer_coordinates[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], \u001b[43manswer_coordinates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m \tanswer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo answer found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "\n",
    "# Example table\n",
    "data = {\n",
    "\t\"Actors\": [\"Brad Pitt\", \"Leonardo DiCaprio\", \"George Clooney\"],\n",
    "\t\"Number of movies\": [\"87\", \"53\", \"69\"]\n",
    "}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Example question\n",
    "queries = [\"How many movies has George Clooney played in?\"]\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer(table=table, queries=queries, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# Get the model outputs\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Convert the logits to predictions\n",
    "logits = outputs.logits.detach()  # Detach the logits tensor\n",
    "predicted_answer_coordinates = tokenizer.convert_logits_to_predictions(\n",
    "\tinputs, logits\n",
    ")\n",
    "\n",
    "# Check if the predicted answer coordinates are valid\n",
    "if predicted_answer_coordinates and predicted_answer_coordinates[0]:\n",
    "\tanswer_coordinates = predicted_answer_coordinates[0]\n",
    "\tanswer = table.iat[answer_coordinates[0][0], answer_coordinates[0][1]]\n",
    "else:\n",
    "\tanswer = \"No answer found\"\n",
    "\n",
    "print(f\"Question: {queries[0]}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### microsoft/tapex-large-finetuned-wtq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'TapexTokenizer'. \n",
      "The class this function is called from is 'TapasTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTapasTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/tapex-large-finetuned-wtq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m TapasForQuestionAnswering\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/tapex-large-finetuned-wtq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Example table\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2032\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2030\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2043\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2272\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2272\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2274\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2277\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/tapas/tokenization_tapas.py:270\u001b[0m, in \u001b[0;36mTapasTokenizer.__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, empty_token, tokenize_chinese_chars, strip_accents, cell_trim_length, max_column_id, max_row_id, strip_column_names, update_answer_coordinates, min_question_length, max_question_length, model_max_length, additional_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     additional_special_tokens \u001b[38;5;241m=\u001b[39m [empty_token]\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a vocabulary file at path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. To load the vocabulary from a Google pretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m load_vocab(vocab_file)\n",
      "File \u001b[0;32m/usr/lib/python3.10/genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "source": [
    "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\n",
    "\n",
    "# Example table\n",
    "data = {\n",
    "\t\"Actors\": [\"Brad Pitt\", \"Leonardo DiCaprio\", \"George Clooney\"],\n",
    "\t\"Number of movies\": [\"87\", \"53\", \"69\"]\n",
    "}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Example question\n",
    "queries = [\"How many movies has George Clooney played in?\"]\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer(table=table, queries=queries, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# Get the model outputs\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Convert the logits to predictions\n",
    "logits = outputs.logits.detach()  # Detach the logits tensor\n",
    "predicted_answer_coordinates = tokenizer.convert_logits_to_predictions(inputs, logits)\n",
    "\n",
    "# Handle empty or invalid predictions\n",
    "if predicted_answer_coordinates and len(predicted_answer_coordinates[0]) > 0:\n",
    "\tanswer_coordinates = predicted_answer_coordinates[0]\n",
    "\n",
    "\t# Ensure coordinates are valid (non-negative and within bounds)\n",
    "\tif all(0 <= coord < len(table) for coord in answer_coordinates[0][0]):\n",
    "\t\tanswer = table.iat[answer_coordinates[0][0][0], answer_coordinates[0][0][1]]\n",
    "\telse:\n",
    "\t\tanswer = \"No valid answer found\"\n",
    "else:\n",
    "\tanswer = \"No answer found\"\n",
    "\n",
    "print(f\"Question: {queries[0]}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### google/tapas-base-finetuned-wtq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vai/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/vai/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/tapas/tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/home/vai/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/tapas/tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many movies has George Clooney played in?\n",
      "Answer: 69\n"
     ]
    }
   ],
   "source": [
    "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "\n",
    "# Example table\n",
    "data = {\n",
    "\t\"Actors\": [\"Brad Pitt\", \"Leonardo DiCaprio\", \"George Clooney\"],\n",
    "\t\"Number of movies\": [\"87\", \"53\", \"69\"]\n",
    "}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Example question\n",
    "queries = [\"How many movies has George Clooney played in?\"]\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer(table=table, queries=queries, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# Get the model outputs\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Convert the logits to predictions\n",
    "logits = outputs.logits.detach()  # Detach the logits tensor\n",
    "predicted_answer_coordinates = tokenizer.convert_logits_to_predictions(inputs, logits)\n",
    "\n",
    "# # Check if the predicted answer coordinates are valid\n",
    "# if predicted_answer_coordinates and predicted_answer_coordinates[0]:\n",
    "# \tanswer_coordinates = predicted_answer_coordinates[0]\n",
    "# \tanswer = table.iat[answer_coordinates[0][0], answer_coordinates[0][1]]\n",
    "# else:\n",
    "# \tanswer = \"No answer found\"\n",
    "\n",
    "# if predicted_answer_coordinates and predicted_answer_coordinates[0]:\n",
    "#     answer_coordinates = predicted_answer_coordinates[0]\n",
    "#     if answer_coordinates:  # Check if there is a valid coordinate\n",
    "#         answer = table.iat[answer_coordinates[0][0], answer_coordinates[0][1]]\n",
    "#     else:\n",
    "#         answer = \"No answer found\"\n",
    "# else:\n",
    "#     answer = \"No answer found\"\n",
    "\n",
    "# Handle empty or invalid predictions\n",
    "if predicted_answer_coordinates and len(predicted_answer_coordinates[0]) > 0:\n",
    "    answer_coordinates = predicted_answer_coordinates[0]\n",
    "\n",
    "    # Ensure coordinates are valid (non-negative and within bounds)\n",
    "    if all(0 <= coord < len(table) for coord in answer_coordinates[0][0]):\n",
    "        answer = table.iat[answer_coordinates[0][0][0], answer_coordinates[0][0][1]]\n",
    "    else:\n",
    "        answer = \"No valid answer found\"\n",
    "else:\n",
    "    answer = \"No answer found\"\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Question: {queries[0]}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helsinki-NLP/opus-mt-en-de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Hallo, wie geht's?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(**inputs)\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facebook/m2m100_418M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['আপনি কোথায় বসবাস করেন?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "en_text = \"Where do you live?\"\n",
    "chinese_text = \"生活就像一盒巧克力。\"\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# translate Hindi to French\n",
    "tokenizer.src_lang = \"en\"\n",
    "encoded_en = tokenizer(en_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.get_lang_id(\"bn\"))\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "# => \"La vie est comme une boîte de chocolat.\"\n",
    "\n",
    "# # translate Chinese to English\n",
    "# tokenizer.src_lang = \"zh\"\n",
    "# encoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\n",
    "# generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "# tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "# => \"Life is like a box of chocolate.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### google-t5/t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "google/t5-small is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/google/t5-small/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:967\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1482\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:302\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 302\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-67691ec8-49dfd20b2251e8737edf747a;ec86fb12-ad99-4f34-9864-b042b69ed69c)\n\nRepository Not Found for url: https://huggingface.co/google/t5-small/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/t5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Use the api_token variable defined in a previous cell\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, use_auth_token\u001b[38;5;241m=\u001b[39mapi_token)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1951\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[1;32m   1949\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 1951\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1965\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1966\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1967\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1968\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   1969\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/utils/hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: google/t5-small is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"google/t5-small\"\n",
    "\n",
    "# Use the api_token variable defined in a previous cell\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, use_auth_token=api_token)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, use_auth_token=api_token)\n",
    "\n",
    "# Example usage\n",
    "text = \"Translate English to German: Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helsinki-NLP/opus-mt-zh-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Hello. How are you?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"你好，你怎么样？\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(**inputs)\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facebook/nllb-200-distilled-600M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'NllbTokenizer'. \n",
      "The class this function is called from is 'M2M100Tokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/nllb-200-distilled-600M\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mM2M100Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m M2M100ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2032\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2030\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2043\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2272\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2272\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2274\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2277\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/m2m_100/tokenization_m2m_100.py:141\u001b[0m, in \u001b[0;36mM2M100Tokenizer.__init__\u001b[0;34m(self, vocab_file, spm_file, src_lang, tgt_lang, bos_token, eos_token, sep_token, pad_token, unk_token, language_codes, sp_model_kwargs, num_madeup_words, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         additional_special_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspm_file \u001b[38;5;241m=\u001b[39m spm_file\n",
      "File \u001b[0;32m~/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/transformers/models/m2m_100/tokenization_m2m_100.py:373\u001b[0m, in \u001b[0;36mload_json\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Dict, List]:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "en_text = \"Where do you live?\"\n",
    "chinese_text = \"生活就像一盒巧克力。\"\n",
    "\n",
    "# translate english to Bangla\n",
    "tokenizer.src_lang = \"en\"\n",
    "encoded_en = tokenizer(en_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.get_lang_id(\"bn\"))\n",
    "translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summerization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facebook/bart-large-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Artificial intelligence (AI) has significantly transformed various industries over the past decade. From healthcare to finance, AI-powered tools have streamlined processes, improved decision-making, and enhanced customer experiences. Despite these advancements, challenges such as ethical considerations, data privacy concerns, and the need for transparency in AI decision- making remain critical issues.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"Artificial intelligence (AI) has significantly transformed various industries over the past decade, offering innovative solutions to complex problems. From healthcare to finance, AI-powered tools have streamlined processes, improved decision-making, and enhanced customer experiences. For instance, machine learning algorithms are being used in hospitals to predict patient outcomes and recommend personalized treatment plans, while financial institutions leverage AI for fraud detection and algorithmic trading. Despite these advancements, challenges such as ethical considerations, data privacy concerns, and the need for transparency in AI decision-making remain critical issues. As technology evolves, addressing these challenges will be crucial to ensure the responsible and equitable use of AI in society.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### google/pegasus-multi_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vai/Desktop/chat_bot_experiment_one/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-multi_news and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: – \"The quick brown fox jumps over the lazy dog.\" That's a sentence you'd expect to see in a dog-eat-dog kind of story, but it turns out it's actually a sentence you'd expect to see in any other English-language article. It's a well-known sentence in English used to test typing, fonts, and keyboards, and it contains every letter in the English alphabet, making it a pangram. \"The quick brown\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"google/pegasus-multi_news\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"The quick brown fox jumps over the lazy dog. This is a well-known sentence in English used to test typing, fonts, and keyboards.\n",
    "          It contains every letter in the English alphabet, making it a pangram. Such sentences are useful for showcasing typefaces.\"\"\"\n",
    "          \n",
    "# Tokenize and process the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"longest\", max_length=512)\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=100, min_length=30, length_penalty=2.0, num_beams=4)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Summary: {summary}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
